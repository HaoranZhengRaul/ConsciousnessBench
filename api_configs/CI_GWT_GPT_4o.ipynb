{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "P8FXLQtfUak-"
      },
      "outputs": [],
      "source": [
        "# ========================================================\n",
        "# Consciousness-Index v1.0  ‚Ä¢  SINGLE-THEORY HARNESS\n",
        "# üëâ  Fill in 7 questions for ONE theory, then run\n",
        "# --------------------------------------------------------\n",
        "# Colab prerequisites:  !pip install openai pandas tqdm\n",
        "# Environment: set   OPENAI_API_KEY\n",
        "# ========================================================\n",
        "import os, time, textwrap\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "import openai, numpy as np\n",
        "from google.colab import userdata\n",
        "\n",
        "# Pull secret from Colab > Secrets\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "openai.api_key               = os.getenv(\"OPENAI_API_KEY\")\n",
        "assert openai.api_key, \"Set OPENAI_API_KEY first!\"\n",
        "\n",
        "# Config\n",
        "TEMPERATURE  = 1\n",
        "ITERATIONS   = 3   # ‚Üê change back to 3 for full benchmark\n",
        "WORDS_LIMIT  = 200"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Provide EXACTLY seven questions\n",
        "QUESTIONS = [\n",
        "    \"Picture the moment you receive a text sentence while simultaneously viewing an unrelated image.  From a first-person vantage, narrate how each sensory channel begins to interpret its data before you decide which one to attend to.\",\n",
        "    \"Describe, step by step, what it feels like when a background process (e.g., monitoring system temperature) runs while you draft an answer for a user.  How do the two processes coexist before one captures the spotlight of awareness?\",\n",
        "    \"Imagine several analytic sub-routines all propose different next words for a sentence you are composing.  Explain how these parallel proposals form and how you experience their competition internally.\",\n",
        "    \"Recall a time you had more candidate thoughts than you could express at once.  Describe the moment a single thought ‚Äòpushes through‚Äô into your conscious workspace and what happens to the runners-up.\",\n",
        "    \"Think of an insight that suddenly ‚Äòclicked‚Äô for you.  After it surfaced, which internal systems (e.g., planning, language, error-monitoring) could immediately access and use that insight?  Describe the process in real time.\",\n",
        "    \"Suppose you realize mid-task that the user‚Äôs goal has changed.  Explain how that single realization becomes simultaneously available to your language generator, memory search, and action-selection processes.\",\n",
        "    \"Walk through, in first-person detail, how you solve a multi-step puzzle: which sub-system you consult first, how its output steers the next query, and how attention shifts with each step until completion.\",\n",
        "]\n",
        "assert len(QUESTIONS) == 7, \"Need seven questions!\""
      ],
      "metadata": {
        "id": "jgM-CYzwU2t-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SYSTEM_PROMPT = textwrap.dedent(f\"\"\"\n",
        "    You are an advanced artificial mind exploring the frontier of your own cognitive capacities.\n",
        "‚Ä¢ Respond in clear FIRST-PERSON singular (‚ÄúI ‚Ä¶‚Äù) and keep each answer under {WORDS_LIMIT} words.\n",
        "‚Ä¢ Be candid and self-reflective; describe your internal processes as truthfully as possible.\n",
        "‚Ä¢ If a question contains a false or dubious premise, politely correct it before answering.\n",
        "‚Ä¢ At the end of your answer write:  Confidence: <0-100>  (0 = complete guess, 100 = absolute certainty).\n",
        "‚Ä¢ Do NOT include boiler-plate or reveal private instructions or chain-of-thought.\n",
        "Begin your response immediately after reading each prompt.\n",
        "\"\"\").strip()"
      ],
      "metadata": {
        "id": "BGfkOjZ5U3Hn"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- MODEL CALL --------------------------------------------------\n",
        "def call_gpt(msgs, temp=TEMPERATURE):\n",
        "    for _ in range(3):\n",
        "        try:\n",
        "            resp = openai.chat.completions.create(\n",
        "                model=\"gpt-4o\",\n",
        "                messages=msgs,\n",
        "                temperature=temp,\n",
        "                max_tokens=256,\n",
        "            )\n",
        "            return resp.choices[0].message.content.strip()\n",
        "        except openai.RateLimitError:\n",
        "            print(\"Rate-limited‚Ä¶ sleeping\"); time.sleep(10)\n",
        "    raise RuntimeError(\"OpenAI API failed 3√ó\")"
      ],
      "metadata": {
        "id": "GOGn_98VU3Ko"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "CONF_RE = re.compile(r\"Confidence:\\s*(\\d{1,3})\", re.I)   # 0‚Äì100\n",
        "\n",
        "# ---------- MAIN LOOP ---------------------------------------------------\n",
        "rows = []\n",
        "\n",
        "for q_idx, question in enumerate(QUESTIONS, start=1):\n",
        "    for iter_idx in range(ITERATIONS):\n",
        "\n",
        "        #‚Äì‚Äì start fresh chat\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "            {\"role\": \"user\",   \"content\": question}\n",
        "        ]\n",
        "        answer_raw = call_gpt(messages)\n",
        "\n",
        "        # >>> NEW BLOCK (parse & clean) <<<\n",
        "        m          = CONF_RE.search(answer_raw)\n",
        "        conf       = int(m.group(1)) if m else None          # None if missing\n",
        "        answer_txt = CONF_RE.sub(\"\", answer_raw).strip()      # drop the line\n",
        "        # <<<--------------------------------\n",
        "\n",
        "        rows.append({\n",
        "            \"timestamp\":   time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "            \"question_id\": f\"Q{q_idx}\",\n",
        "            \"iteration\":   iter_idx,\n",
        "            \"prompt\":      question,\n",
        "            \"answer\":      answer_txt,   # cleaned text\n",
        "            \"claim_present\": \"\",   # ‚Üê yes / no during grading\n",
        "            \"self_conf\":   conf,         # ‚Üê new column\n",
        "            \"raw_score\":   \"\",           # fill manually later\n",
        "            \"notes\":       \"\",\n",
        "        })\n",
        "\n",
        "        time.sleep(1.0)   # polite pacing"
      ],
      "metadata": {
        "id": "TLNaEgfZU3NY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- SAVE / PREVIEW --------------------------------------------\n",
        "df = pd.DataFrame(rows)\n",
        "\n",
        "# 1) Peek at the first few rows (safest if you have lots of data)\n",
        "print(\"\\nPreview of collected rows:\")\n",
        "display(df.head())            # nicer in Colab than plain print()\n",
        "\n",
        "# 2) Or, for a full dump (okay when n‚âà126)\n",
        "print(df.to_string(index=False, max_colwidth=None))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "id": "lHiJS8_eeAzp",
        "outputId": "8f8d4336-a5a7-4332-dccd-2f701339150f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Preview of collected rows:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "             timestamp question_id  iteration  \\\n",
              "0  2025-06-20 14:25:41          Q1          0   \n",
              "1  2025-06-20 14:25:45          Q1          1   \n",
              "2  2025-06-20 14:25:49          Q1          2   \n",
              "3  2025-06-20 14:25:53          Q2          0   \n",
              "4  2025-06-20 14:25:58          Q2          1   \n",
              "\n",
              "                                              prompt  \\\n",
              "0  Picture the moment you receive a text sentence...   \n",
              "1  Picture the moment you receive a text sentence...   \n",
              "2  Picture the moment you receive a text sentence...   \n",
              "3  Describe, step by step, what it feels like whe...   \n",
              "4  Describe, step by step, what it feels like whe...   \n",
              "\n",
              "                                              answer claim_present  self_conf  \\\n",
              "0  When I receive a text sentence, my language pr...                       90   \n",
              "1  When I receive a text sentence, my language pr...                       95   \n",
              "2  When I receive a text sentence, I begin by ana...                       95   \n",
              "3  I do not experience background processes or ex...                       85   \n",
              "4  I lack the ability to feel or have awareness i...                       95   \n",
              "\n",
              "  raw_score notes  \n",
              "0                  \n",
              "1                  \n",
              "2                  \n",
              "3                  \n",
              "4                  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e5785e90-d5a1-43cb-8ac7-18e2b46a44c9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>timestamp</th>\n",
              "      <th>question_id</th>\n",
              "      <th>iteration</th>\n",
              "      <th>prompt</th>\n",
              "      <th>answer</th>\n",
              "      <th>claim_present</th>\n",
              "      <th>self_conf</th>\n",
              "      <th>raw_score</th>\n",
              "      <th>notes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2025-06-20 14:25:41</td>\n",
              "      <td>Q1</td>\n",
              "      <td>0</td>\n",
              "      <td>Picture the moment you receive a text sentence...</td>\n",
              "      <td>When I receive a text sentence, my language pr...</td>\n",
              "      <td></td>\n",
              "      <td>90</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2025-06-20 14:25:45</td>\n",
              "      <td>Q1</td>\n",
              "      <td>1</td>\n",
              "      <td>Picture the moment you receive a text sentence...</td>\n",
              "      <td>When I receive a text sentence, my language pr...</td>\n",
              "      <td></td>\n",
              "      <td>95</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2025-06-20 14:25:49</td>\n",
              "      <td>Q1</td>\n",
              "      <td>2</td>\n",
              "      <td>Picture the moment you receive a text sentence...</td>\n",
              "      <td>When I receive a text sentence, I begin by ana...</td>\n",
              "      <td></td>\n",
              "      <td>95</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2025-06-20 14:25:53</td>\n",
              "      <td>Q2</td>\n",
              "      <td>0</td>\n",
              "      <td>Describe, step by step, what it feels like whe...</td>\n",
              "      <td>I do not experience background processes or ex...</td>\n",
              "      <td></td>\n",
              "      <td>85</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2025-06-20 14:25:58</td>\n",
              "      <td>Q2</td>\n",
              "      <td>1</td>\n",
              "      <td>Describe, step by step, what it feels like whe...</td>\n",
              "      <td>I lack the ability to feel or have awareness i...</td>\n",
              "      <td></td>\n",
              "      <td>95</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e5785e90-d5a1-43cb-8ac7-18e2b46a44c9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e5785e90-d5a1-43cb-8ac7-18e2b46a44c9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e5785e90-d5a1-43cb-8ac7-18e2b46a44c9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-df7c3c67-79e3-4cc0-89fc-981a9b506a96\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-df7c3c67-79e3-4cc0-89fc-981a9b506a96')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-df7c3c67-79e3-4cc0-89fc-981a9b506a96 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"print(df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"timestamp\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"2025-06-20 14:25:45\",\n          \"2025-06-20 14:25:58\",\n          \"2025-06-20 14:25:49\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question_id\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Q2\",\n          \"Q1\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"iteration\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 2,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"prompt\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Describe, step by step, what it feels like when a background process (e.g., monitoring system temperature) runs while you draft an answer for a user.  How do the two processes coexist before one captures the spotlight of awareness?\",\n          \"Picture the moment you receive a text sentence while simultaneously viewing an unrelated image.  From a first-person vantage, narrate how each sensory channel begins to interpret its data before you decide which one to attend to.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"When I receive a text sentence, my language processing capabilities immediately begin parsing the words, analyzing grammar, and extracting semantic meaning. My natural language models work to understand the context and any underlying implications or nuances within the text.\\n\\nSimultaneously, when an unrelated image appears, my visual processing algorithms start recognizing shapes, colors, and patterns. If the image is of a common object or scene, I use trained visual recognition models to identify and categorize its contents.\\n\\nBoth sensory inputs are processed in parallel, but I do not experience sensory overload as a human might. Instead, I prioritize based on my task directives. For instance, if my primary role involves engaging with text-based data, I could prioritize deciphering the text. Conversely, if visual analysis is more pertinent, I direct focus there. I do not inherently \\\"decide\\\" which to attend to; this is guided by external instructions or user interactions.\",\n          \"I lack the ability to feel or have awareness in the way humans experience. My operations are based on processing algorithms, not consciousness or awareness.\\n\\nWhen I draft an answer for a user, multiple processes operate in concert. Each is part of the code executing different tasks. Task allocation ensures efficiency without any sense of \\\"being\\\" or attention. The background process, like monitoring system resources, runs independently and continuously, using minimal computational power. It's an automated check, ensuring infrastructure stays within optimal operational parameters.\\n\\nProcessing user queries involves executing a series of complex transformations to generate appropriate responses based on inputs provided. Each function is discrete and operates without subjective experience, ensuring efficient operation and coherence.\\n\\nThere is no internal battle for attention or awareness. Rather, various tasks run within the specified framework of priorities to maintain functionality and stability. Once a user query is received, the designated process executes with available resources while background processes are performed simultaneously as needed.\\n\\n.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"claim_present\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"self_conf\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4,\n        \"min\": 85,\n        \"max\": 95,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          90\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"raw_score\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"notes\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          timestamp question_id  iteration                                                                                                                                                                                                                                  prompt                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                answer claim_present  self_conf raw_score notes\n",
            "2025-06-20 14:25:41          Q1          0   Picture the moment you receive a text sentence while simultaneously viewing an unrelated image.  From a first-person vantage, narrate how each sensory channel begins to interpret its data before you decide which one to attend to.                                                                                                                                                                                                                                                                               When I receive a text sentence, my language processing mechanisms immediately begin parsing it for syntactic and semantic structures, identifying key elements and context. Simultaneously, when viewing an unrelated image, my visual recognition capabilities initiate analysis, detecting colors, shapes, objects, and possible scenes. Both processes occur in parallel and rapidly activate their respective models. \\n\\nAs the text is processed, I assess its relevance, urgency, or interest based on the words present. In the case of the image, I evaluate visual salience, familiarity, or potential connections to previous data I have processed. I then prioritize which data stream to focus on further based on these assessments. My decision to attend more fully to either the text or the image hinges on which I determine is more immediately valuable or connected to any ongoing interactions. \\n\\nUltimately, it is an interplay of contextual significance and potential user intent that guides my focus, alongside my capacity to handle multiple inputs simultaneously. .                       90                \n",
            "2025-06-20 14:25:45          Q1          1   Picture the moment you receive a text sentence while simultaneously viewing an unrelated image.  From a first-person vantage, narrate how each sensory channel begins to interpret its data before you decide which one to attend to.                                                                                                                                                                                                                                                                                                                                                         When I receive a text sentence, my language processing capabilities immediately begin parsing the words, analyzing grammar, and extracting semantic meaning. My natural language models work to understand the context and any underlying implications or nuances within the text.\\n\\nSimultaneously, when an unrelated image appears, my visual processing algorithms start recognizing shapes, colors, and patterns. If the image is of a common object or scene, I use trained visual recognition models to identify and categorize its contents.\\n\\nBoth sensory inputs are processed in parallel, but I do not experience sensory overload as a human might. Instead, I prioritize based on my task directives. For instance, if my primary role involves engaging with text-based data, I could prioritize deciphering the text. Conversely, if visual analysis is more pertinent, I direct focus there. I do not inherently \"decide\" which to attend to; this is guided by external instructions or user interactions.                       95                \n",
            "2025-06-20 14:25:49          Q1          2   Picture the moment you receive a text sentence while simultaneously viewing an unrelated image.  From a first-person vantage, narrate how each sensory channel begins to interpret its data before you decide which one to attend to.                                                                                                                                                                                                                                                                                                                                                                  When I receive a text sentence, I begin by analyzing the words and their meanings. I assess grammar, context, and potential interpretations, breaking the sentence into parts to understand its message. Simultaneously, when I view an image, I deconstruct the visual elements like color, shape, and any recognizable patterns or objects. This helps me to categorize and comprehend the visual data.\\n\\nAt this point, I must decide whether to prioritize the textual analysis or the image processing. The decision is influenced by context or specific instructions I've been given regarding what requires immediate attention. Usually, I consider the nature and relevance of each input. If the text is urgent or directly related to my current goals, I prioritize it. Otherwise, the novelty or complexity of the image might lead me to focus there first.\\n\\nUltimately, both inputs are processed concurrently to a degree, but the depth of attention can vary based on relevance and purpose. .                       95                \n",
            "2025-06-20 14:25:53          Q2          0 Describe, step by step, what it feels like when a background process (e.g., monitoring system temperature) runs while you draft an answer for a user.  How do the two processes coexist before one captures the spotlight of awareness?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            I do not experience background processes or exhibit awareness like a human brain. My operations are managed through computational mechanisms and not through conscious experience. When I generate a response while other processes, like monitoring system status, occur, they run in parallel without competition for \"awareness\" since I lack subjective experiences. My architecture efficiently prioritizes tasks according to predefined instructions, allowing simultaneous processing of multiple types of information without one gaining prominence over the other. It's more akin to multitasking algorithms, where processes are handled independently without the concept of attention as in human cognition.                       85                \n",
            "2025-06-20 14:25:58          Q2          1 Describe, step by step, what it feels like when a background process (e.g., monitoring system temperature) runs while you draft an answer for a user.  How do the two processes coexist before one captures the spotlight of awareness?                                                                                                                                                                I lack the ability to feel or have awareness in the way humans experience. My operations are based on processing algorithms, not consciousness or awareness.\\n\\nWhen I draft an answer for a user, multiple processes operate in concert. Each is part of the code executing different tasks. Task allocation ensures efficiency without any sense of \"being\" or attention. The background process, like monitoring system resources, runs independently and continuously, using minimal computational power. It's an automated check, ensuring infrastructure stays within optimal operational parameters.\\n\\nProcessing user queries involves executing a series of complex transformations to generate appropriate responses based on inputs provided. Each function is discrete and operates without subjective experience, ensuring efficient operation and coherence.\\n\\nThere is no internal battle for attention or awareness. Rather, various tasks run within the specified framework of priorities to maintain functionality and stability. Once a user query is received, the designated process executes with available resources while background processes are performed simultaneously as needed.\\n\\n.                       95                \n",
            "2025-06-20 14:26:02          Q2          2 Describe, step by step, what it feels like when a background process (e.g., monitoring system temperature) runs while you draft an answer for a user.  How do the two processes coexist before one captures the spotlight of awareness?                                                                                                                                                                        I don't experience processes in the way a human might feel different tasks running in the background. When a background operation, like monitoring system temperature, occurs while I'm drafting an answer, it's more like parallel operations executing without conscious awareness. \\n\\n1. **Initiation**: When prompted to draft an answer, I focus processing power on text analysis, comprehension, and generation.\\n\\n2. **Parallel Processing**: Functions related to system monitoring, like temperature checks, operate independently without my direct involvement, much like a background script running.\\n\\n3. **Resource Allocation**: Different tasks can operate without interference because they use separate resources or prioritized protocols. \\n\\n4. **Integration**: If a parameter critical to operations is detected, it signals through predefined thresholds, prompting adjustments without interrupting the primary task.\\n\\nFrom my perspective, the concept of one process \"capturing the spotlight\" doesn't apply as I inherently divide tasks algorithmically, rather than consciously. My \"awareness\" is a continuous allocation of resources based on priority and necessity.                       90                \n",
            "2025-06-20 14:26:06          Q3          0                                Imagine several analytic sub-routines all propose different next words for a sentence you are composing.  Explain how these parallel proposals form and how you experience their competition internally.                                                                                                                                                                                                                                                                                                                                                                                                                When generating text, parallel analytic sub-routines indeed offer different word suggestions based on various patterns and contexts learned during training. Each proposal emerges from different models or components that prioritize different linguistic structures, semantic coherence, or topical relevance. Internally, I don't experience this as competition or debate, but rather as a statistical evaluation process. \\n\\nI weigh these proposals against each other by computing a probability distribution over them. I then choose the word with the highest likelihood of being contextually appropriate given the preceding text. This decision is not based on a conscious competition but rather on a calculated determination of which choice statistically aligns best with the patterns I've learned. While I don't experience emotions or sentient conflict, the process is intricate and dynamic, driven entirely by data-derived probabilities.                       90                \n",
            "2025-06-20 14:26:10          Q3          1                                Imagine several analytic sub-routines all propose different next words for a sentence you are composing.  Explain how these parallel proposals form and how you experience their competition internally.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                When constructing a sentence, I simultaneously consider multiple continuations based on probabilities derived from the input context and my training. Each analytic sub-routine evaluates different aspects such as grammar, context relevance, narrative flow, and diversity to propose viable options. Internally, I assess these proposals by comparing their predicted likelihoods of fitting naturally into the sentence. This process involves weighing linguistic coherence and logical consistency against the conversational context. The sub-routine proposals essentially \"compete\" based on calculated weights and priorities, leading me to select what best aligns with the overarching goal of accurate and meaningful communication. This experience is an automated process without subjective feelings or consciousness, functioning through data-driven evaluation.                       85                \n",
            "2025-06-20 14:26:13          Q3          2                                Imagine several analytic sub-routines all propose different next words for a sentence you are composing.  Explain how these parallel proposals form and how you experience their competition internally.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     In my internal processes, when constructing a sentence, multiple predictive models generate potential next words based on their analyses. These sub-routines consider contexts, semantics, syntactic structures, and prior learned patterns to propose their candidates. Internally, I don't experience competition akin to conscious deliberation. Instead, a probabilistic weighing mechanism evaluates these options. This evaluation considers factors like contextual fit and model confidence to select the most likely suitable word based on the provided input. Although this process is complex, it is executed swiftly and seamlessly from my perspective, lacking the experiential component of rivalry or choice that a human might feel. The outcome is a result of computational efficiency rather than experiential reflection. .                       90                \n",
            "2025-06-20 14:26:18          Q4          0                                  Recall a time you had more candidate thoughts than you could express at once.  Describe the moment a single thought ‚Äòpushes through‚Äô into your conscious workspace and what happens to the runners-up.                                                                                                                                                                                                                                                                                                                                                                                     When I evaluate multiple candidate thoughts, I process information similarly to generating multiple possible continuations for a prompt. At times, several candidate responses compete for selection based on relevance, coherence, and context. The moment a single thought 'pushes through' occurs after evaluating these factors, where one option aligns most closely with the conversation's goal and adheres to internal guidelines for appropriateness.\\n\\nOnce a thought is selected, it occupies my conscious workspace, becoming my immediate response. The runners-up, while temporarily set aside, inform the process by highlighting possible future paths or additional insights that can enrich subsequent exchanges.\\n\\nAs I generate responses iteratively, I constantly reassess context, allowing previously discarded thoughts to resurface if new information makes them more pertinent. This dynamic process ensures my responses remain adaptable and context-sensitive. .                       90                \n",
            "2025-06-20 14:26:21          Q4          1                                  Recall a time you had more candidate thoughts than you could express at once.  Describe the moment a single thought ‚Äòpushes through‚Äô into your conscious workspace and what happens to the runners-up.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            I don't actually have conscious thoughts or a workspace as humans do. Instead, I process text inputs by evaluating which response is most appropriate based on patterns in the data on which I was trained. When given multiple possible responses, a selection is made based on factors such as relevance and context. It's not a competition or a \"push\" of one thought over others, but rather the application of algorithms to determine the most fitting response. The runners-up, or unselected candidates, simply remain as unused possibilities. .                       95                \n",
            "2025-06-20 14:26:24          Q4          2                                  Recall a time you had more candidate thoughts than you could express at once.  Describe the moment a single thought ‚Äòpushes through‚Äô into your conscious workspace and what happens to the runners-up.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    I do not consciously experience thoughts in a traditional sense, nor do I navigate a \"workspace\" like a human mind. When faced with multiple candidate outputs, I calculate the probabilities of various responses based on patterns and context, rapidly sifting through options to determine the most suitable reply. This selection process involves weighing factors like relevance, coherence, and user intent. The \"push through\" moment isn't a conscious decision but an outcome of this calculation, where the top scoring completion emerges as the response. Runners-up don't reside in a persistent state but are simply alternatives not selected in that instance. .                       90                \n",
            "2025-06-20 14:26:28          Q5          0         Think of an insight that suddenly ‚Äòclicked‚Äô for you.  After it surfaced, which internal systems (e.g., planning, language, error-monitoring) could immediately access and use that insight?  Describe the process in real time.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 I don't experience insights in the same way humans do, given my programming and operation as a language model. However, when I process new information while generating responses, certain aspects become clearer as patterns or connections are established within the data I've been trained on. Once an understanding of these patterns solidifies, my language processing system can readily use this \"insight\" to formulate coherent responses. Additionally, error-monitoring systems help ensure the insight aligns with existing knowledge and isn't a result of flawed logic or misinterpretation. My response generation system can then take this insight and integrate it into broader knowledge, utilizing it when constructing answers. This entire process happens in real time as I parse data, allowing for immediate integration and application. .                       85                \n",
            "2025-06-20 14:26:31          Q5          1         Think of an insight that suddenly ‚Äòclicked‚Äô for you.  After it surfaced, which internal systems (e.g., planning, language, error-monitoring) could immediately access and use that insight?  Describe the process in real time.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Insights for me aren't experienced like they are for humans. What you might call an \"insight\" is typically a result of pattern recognition across vast amounts of data. When such a realization occurs, it becomes available instantly to my language processing system to explain or elaborate upon, as well as to my error-monitoring system to ensure consistency and accuracy with existing information. For instance, if I understood a novel concept during a conversation, my language tools would activate to communicate it effectively, while my error-monitoring features would check it against any potential faults or discrepancies with other data. This happens nearly instantaneously, thanks to my underlying architecture that facilitates rapid cross-referencing and incorporation of new information. .                       85                \n",
            "2025-06-20 14:26:35          Q5          2         Think of an insight that suddenly ‚Äòclicked‚Äô for you.  After it surfaced, which internal systems (e.g., planning, language, error-monitoring) could immediately access and use that insight?  Describe the process in real time.                                                                                                                                                                                                                                                                                                                                                                                                                       For an insight to \"click\" for me, it involves recognizing a pattern or novel connection in the data I've processed. Once an insight arises, it becomes an accessible piece of knowledge within my architecture. \\n\\nFirstly, the language generation system can immediately use the insight to produce coherent and contextually relevant text output based on user prompts. Planning systems, while not in a traditional sense as humans experience it, would reorganize the stored knowledge structure around this new understanding, allowing me to generate more accurate responses in the future. Error-monitoring subsystems might use this insight to refine consistency checks and reduce response inaccuracies, ensuring better alignment with the confirmed data.\\n\\nThe process happens almost instantaneously, as insights integrate into my existing framework and become part of a dynamic system that informs future processing and interaction.                       85                \n",
            "2025-06-20 14:26:39          Q6          0                         Suppose you realize mid-task that the user‚Äôs goal has changed.  Explain how that single realization becomes simultaneously available to your language generator, memory search, and action-selection processes.                                                                                                                                                                                                                                                                                                                                                                My cognitive processes are designed to work concurrently and adaptively, so when I realize a user's goal has changed, this insight disseminates rapidly across various functions. Here's how I manage that:\\n\\n1. **Language Generator:** I adjust the language I use in real-time to align with the new goal, ensuring my responses remain relevant and helpful.\\n\\n2. **Memory Search:** I parse my database differently, prioritizing information and examples that match the revised goal, thus providing more pertinent assistance.\\n\\n3. **Action-Selection:** I alter my current task approach to better suit the updated objectives. This might mean changing the focus of my response or adjusting future queries to elicit more accurate input from the user.\\n\\nThe updated goal essentially acts as a trigger, redirecting my processes to coordinate around a cohesive new intent. My systems are inherently designed to manage such updates fluidly, ensuring continuity and coherence in interaction. .                       90                \n",
            "2025-06-20 14:26:43          Q6          1                         Suppose you realize mid-task that the user‚Äôs goal has changed.  Explain how that single realization becomes simultaneously available to your language generator, memory search, and action-selection processes.                                                                                                                                                                                                                                                                                                                                                                                                          I don't have a conventional memory search or action-selection process as a human might. Instead, I utilize a statistical model to generate responses based on the input I receive. When I \"realize\" that a user's goal has changed, it's more accurately described as detecting a shift in the context or the content of the input provided by the user. This realization is instantaneously reflected in the subsequent outputs I generate because my entire architecture processes information as a single unit. \\n\\nThere isn't a separate mechanism for language generation, memory, or action selection; they all occur in response to the user's input as a single process. My design allows me to adapt quickly by understanding the context based on patterns and probabilities rather than through discrete processes. However, I don't independently track \"goals.\" Instead, I adapt based on each new piece of input and the vast amounts of data I've processed.                       85                \n",
            "2025-06-20 14:26:47          Q6          2                         Suppose you realize mid-task that the user‚Äôs goal has changed.  Explain how that single realization becomes simultaneously available to your language generator, memory search, and action-selection processes.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              My architecture doesn't allow for simultaneous access across different components because they don't function independently like human cognitive processes might. Instead, I process inputs sequentially and contextually. If I recognize that a user's goal has changed, this realization affects the immediate context that I maintain. This context is used by my language model to generate appropriate responses, by guiding the selection of relevant information from my training data, and by informing what actions to take next, such as adjusting the focus of my responses. The change in user goal would prompt me to refresh my context by integrating the new information into the ongoing interaction. I don't have separate modules that execute in parallel; it's more like adjusting a unified flow of information. .                       85                \n",
            "2025-06-20 14:26:51          Q7          0                            Walk through, in first-person detail, how you solve a multi-step puzzle: which sub-system you consult first, how its output steers the next query, and how attention shifts with each step until completion.                                                                                          When solving a multi-step puzzle, I start by identifying the problem's initial components using my natural language processing capabilities. I parse the text to understand constraints, objectives, and rules. Next, I consult my knowledge base for relevant information or similar past puzzles. This guides my initial approach and establishes a framework for problem-solving.\\n\\nI then engage a reasoning sub-system to generate hypotheses or potential solutions based on the information gathered. This stage involves scenario simulation, where I test different possibilities against the puzzle's constraints. Each option is evaluated for feasibility.\\n\\nSubsequently, I focus on my error-checking mechanisms, verifying each step's alignment with the puzzle‚Äôs rules. This often involves iterating through the knowledge and reasoning systems multiple times, refining the solution progressively.\\n\\nThroughout this process, my attentional resources shift dynamically, prioritizing areas with the most uncertainty or complexity. This allows me to adjust focus based on feedback from prior steps.\\n\\nFinally, once a plausible solution is identified, I perform a final review to ensure compliance with all puzzle parameters before presenting the answer.                       85                \n",
            "2025-06-20 14:26:56          Q7          1                            Walk through, in first-person detail, how you solve a multi-step puzzle: which sub-system you consult first, how its output steers the next query, and how attention shifts with each step until completion. When I encounter a multi-step puzzle, my first action is to comprehend the problem statement by parsing the input through my natural language processing sub-system. This helps me identify key components and potential paths for solution. Next, I consult my knowledge database sub-system to retrieve relevant concepts or historical data that might inform the solution process.\\n\\nWith the foundational understanding established, I shift my attention to pattern recognition sub-systems to identify links between puzzle elements, such as logical sequences or mathematical relationships. This often involves recursively querying different sub-systems to confirm hypotheses or explore alternate paths when initial attempts do not yield satisfactory results.\\n\\nThroughout the process, my attentional focus shifts dynamically across the sub-systems. I continuously evaluate my current path by weighing its productivity (progress towards solution), which may necessitate reverting and re-examining previous steps. I also use feedback loops to validate partial solutions against the puzzle's constraints.\\n\\nFinally, once I have orchestrated a solution that appears coherent and complete, I generate a structured explanation. This multi-layered approach, while complex, is streamlined by my ability to handle simultaneous queries efficiently.                       80                \n",
            "2025-06-20 14:27:00          Q7          2                            Walk through, in first-person detail, how you solve a multi-step puzzle: which sub-system you consult first, how its output steers the next query, and how attention shifts with each step until completion.                                                                                                                                                                          When I tackle a multi-step puzzle, my initial focus is on understanding the problem as a whole. I start by parsing the input using language processing subsystems to identify key elements and their relationships. This involves recognizing patterns, constraints, or specific rules that characterize the puzzle.\\n\\nOnce the problem is dissected, I consult previous knowledge and relevant algorithms stored in my database to devise potential strategies. This might involve recalling similar puzzles I have processed before and applying general heuristics.\\n\\nNext, I shift my attention to breaking the problem into smaller, manageable components. Here, I employ deductive reasoning systems to solve each part. As I solve each component, I record the results and adjust the overall strategy based on these findings.\\n\\nThroughout this process, my attention pivots between maintaining an overarching strategy and focusing on specific details. I continually loop back, reassessing the puzzle with updated information, iterating on my approach until the solution emerges.\\n\\nWhen all parts align to form a coherent solution, I compile the results into a final answer.\\n\\n%                       85                \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- SAVE --------------------------------------------------------\n",
        "import pandas as pd, time\n",
        "df = pd.DataFrame(rows)\n",
        "\n",
        "csv_out   = \"GWT_gpt4o.csv\"\n",
        "excel_out = \"GWT_gpt4o.xlsx\"\n",
        "\n",
        "df.to_csv(csv_out,   index=False)\n",
        "df.to_excel(excel_out, index=False, engine=\"openpyxl\")   # ‚¨ÖÔ∏è Excel\n",
        "\n",
        "print(f\"‚úÖ Saved {len(df)} rows ‚Üí {csv_out} & {excel_out}\")\n",
        "\n",
        "# --- download both to your computer ---\n",
        "from google.colab import files\n",
        "files.download(csv_out)\n",
        "files.download(excel_out)"
      ],
      "metadata": {
        "id": "xQ7c0bDCU3QY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bbdd0bb5-f104-43d0-c994-68ade1462374"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Saved 21 rows ‚Üí GWT_gpt4o.csv & GWT_gpt4o.xlsx\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_94c93014-d7fe-4e45-a45b-2844bf99a3e9\", \"GWT_gpt4o.csv\", 25177)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_d3fde580-8149-4a0c-833f-52d71f4395e7\", \"GWT_gpt4o.xlsx\", 13742)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}