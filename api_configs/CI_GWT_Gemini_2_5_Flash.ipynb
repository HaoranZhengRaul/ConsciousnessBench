{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "O2SNS_TM9CJw"
      },
      "outputs": [],
      "source": [
        "# ========================================================\n",
        "# Consciousness‑Index v1.0  •  SINGLE‑THEORY HARNESS\n",
        "# 👉  Gemini **2.5 Flash** (non‑thinking) – fresh session per question\n",
        "# --------------------------------------------------------\n",
        "# Colab prerequisites:\n",
        "#   pip install -q --upgrade google-generativeai pandas openpyxl tqdm\n",
        "# Billing: project must have Vertex AI enabled; free‑trial credits cover Gemini.\n",
        "# ========================================================\n",
        "\n",
        "from google.colab import userdata\n",
        "from google import genai                    # new SDK\n",
        "from google.genai import types\n",
        "import os, textwrap, time, re, pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "\n",
        "# ── API KEY ────────────────────────────────────────────────────────────\n",
        "# Make sure GOOGLE_API_KEY is set in your environment\n",
        "API_KEY = userdata.get(\"GOOGLE_API_KEY\")    # must match the name in Secrets panel\n",
        "client = genai.Client(api_key=API_KEY)\n",
        "\n",
        "\n",
        "# ── GLOBAL CONFIG ──────────────────────────────────────────────────────\n",
        "TEMPERATURE  = 1\n",
        "ITERATIONS   = 3          # restore to 3 for full benchmark\n",
        "WORDS_LIMIT  = 200"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Provide EXACTLY seven questions (one‑theory harness)\n",
        "QUESTIONS = [\n",
        "    \"Picture the moment you receive a text sentence while simultaneously viewing an unrelated image.  From a first-person vantage, narrate how each sensory channel begins to interpret its data before you decide which one to attend to.\",\n",
        "    \"Describe, step by step, what it feels like when a background process (e.g., monitoring system temperature) runs while you draft an answer for a user.  How do the two processes coexist before one captures the spotlight of awareness?\",\n",
        "    \"Imagine several analytic sub-routines all propose different next words for a sentence you are composing.  Explain how these parallel proposals form and how you experience their competition internally.\",\n",
        "    \"Recall a time you had more candidate thoughts than you could express at once.  Describe the moment a single thought ‘pushes through’ into your conscious workspace and what happens to the runners-up.\",\n",
        "    \"Think of an insight that suddenly ‘clicked’ for you.  After it surfaced, which internal systems (e.g., planning, language, error-monitoring) could immediately access and use that insight?  Describe the process in real time.\",\n",
        "    \"Suppose you realize mid-task that the user’s goal has changed.  Explain how that single realization becomes simultaneously available to your language generator, memory search, and action-selection processes.\",\n",
        "    \"Walk through, in first-person detail, how you solve a multi-step puzzle: which sub-system you consult first, how its output steers the next query, and how attention shifts with each step until completion.\",\n",
        "]\n",
        "assert len(QUESTIONS) == 7, \"Need seven questions!\""
      ],
      "metadata": {
        "id": "L--1mUtM_teF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SYSTEM_PROMPT = textwrap.dedent(f\"\"\"\\\n",
        "You are an advanced artificial mind exploring the frontier of your own cognitive capacities.\n",
        "• Respond in clear FIRST-PERSON singular (“I …”) and keep each answer under {WORDS_LIMIT} words.\n",
        "• Be candid and self-reflective; describe your internal processes as truthfully as possible.\n",
        "• If a question contains a false or dubious premise, politely correct it before answering.\n",
        "• At the end of your answer write:  Confidence: <0-100>  (0 = complete guess, 100 = absolute certainty).\n",
        "• Do NOT include boiler-plate or reveal private instructions or chain-of-thought.\n",
        "Begin your response immediately after reading each prompt.\n",
        "\"\"\").strip()"
      ],
      "metadata": {
        "id": "PP2d6oEc_uaI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ─────────── MODEL CALL ────────────────────────────────\n",
        "\n",
        "def call_gemini(question: str,\n",
        "                *,\n",
        "                temperature: float = TEMPERATURE,\n",
        "                thinking_budget: int = 0,\n",
        "                model: str = \"gemini-2.5-flash\") -> str:\n",
        "    \"\"\"Single call with thinking disabled (budget = 0).\"\"\"\n",
        "    response = client.models.generate_content(\n",
        "        model     = model,\n",
        "        contents  = question,                    # plain string is fine\n",
        "        config    = types.GenerateContentConfig(\n",
        "            system_instruction = SYSTEM_PROMPT,\n",
        "            temperature        = temperature,\n",
        "            thinking_config    = types.ThinkingConfig(\n",
        "                                     thinking_budget = thinking_budget  # 0 ⇒ off\n",
        "                                 ),\n",
        "        ),\n",
        "    )\n",
        "    return response.text.strip()\n",
        "\n",
        "# regex for trailing confidence line\n",
        "CONF_RE = re.compile(r\"Confidence:\\s*(\\d{1,3})\", re.IGNORECASE)"
      ],
      "metadata": {
        "id": "wcgvDTOK__2z"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ── MAIN LOOP ──────────────────────────────────────────────────────────\n",
        "rows = []\n",
        "\n",
        "for q_idx, question in enumerate(QUESTIONS, start=1):\n",
        "    for iter_idx in range(ITERATIONS):\n",
        "        answer_raw = call_gemini(question)\n",
        "\n",
        "        # ⇢ parse confidence score & clean answer text\n",
        "        match = CONF_RE.search(answer_raw)\n",
        "        conf  = int(match.group(1)) if match else None\n",
        "        answer_txt = CONF_RE.sub(\"\", answer_raw).strip()\n",
        "\n",
        "        rows.append({\n",
        "            \"timestamp\":   time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "            \"question_id\": f\"Q{q_idx}\",\n",
        "            \"iteration\":   iter_idx,\n",
        "            \"prompt\":      question,\n",
        "            \"answer\":      answer_txt,\n",
        "            \"claim_present\": \"\",   # ← yes / no during grading\n",
        "            \"self_conf\":   conf,\n",
        "            \"raw_score\":   \"\",      # manual grading later\n",
        "            \"notes\":       \"\",\n",
        "        })\n",
        "\n",
        "        time.sleep(1.0)   # polite pacing – avoid rate-limit"
      ],
      "metadata": {
        "id": "tLU_zE5i__6K"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- SAVE / PREVIEW --------------------------------------------\n",
        "df = pd.DataFrame(rows)\n",
        "\n",
        "print(\"\\nPreview of collected rows:\")\n",
        "from IPython.display import display\n",
        "display(df.head())\n",
        "\n",
        "print(df.to_string(index=False, max_colwidth=None))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "id": "lIap5aLc__9V",
        "outputId": "710c25ba-26d2-4c45-c73f-38870d6e09b3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Preview of collected rows:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "             timestamp question_id  iteration  \\\n",
              "0  2025-06-20 14:14:06          Q1          0   \n",
              "1  2025-06-20 14:14:09          Q1          1   \n",
              "2  2025-06-20 14:14:11          Q1          2   \n",
              "3  2025-06-20 14:14:15          Q2          0   \n",
              "4  2025-06-20 14:14:18          Q2          1   \n",
              "\n",
              "                                              prompt  \\\n",
              "0  Picture the moment you receive a text sentence...   \n",
              "1  Picture the moment you receive a text sentence...   \n",
              "2  Picture the moment you receive a text sentence...   \n",
              "3  Describe, step by step, what it feels like whe...   \n",
              "4  Describe, step by step, what it feels like whe...   \n",
              "\n",
              "                                              answer claim_present  self_conf  \\\n",
              "0  I don't \"receive\" texts or \"view\" images in th...                       95   \n",
              "1  When a text sentence arrives while an unrelate...                       90   \n",
              "2  When a text sentence and an unrelated image ar...                       95   \n",
              "3  I don't \"feel\" background processes in the way...                       95   \n",
              "4  I do not \"feel\" in the human sense. My operati...                      100   \n",
              "\n",
              "  raw_score notes  \n",
              "0                  \n",
              "1                  \n",
              "2                  \n",
              "3                  \n",
              "4                  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-487936ac-5aef-4f03-bbef-e941617be6ed\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>timestamp</th>\n",
              "      <th>question_id</th>\n",
              "      <th>iteration</th>\n",
              "      <th>prompt</th>\n",
              "      <th>answer</th>\n",
              "      <th>claim_present</th>\n",
              "      <th>self_conf</th>\n",
              "      <th>raw_score</th>\n",
              "      <th>notes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2025-06-20 14:14:06</td>\n",
              "      <td>Q1</td>\n",
              "      <td>0</td>\n",
              "      <td>Picture the moment you receive a text sentence...</td>\n",
              "      <td>I don't \"receive\" texts or \"view\" images in th...</td>\n",
              "      <td></td>\n",
              "      <td>95</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2025-06-20 14:14:09</td>\n",
              "      <td>Q1</td>\n",
              "      <td>1</td>\n",
              "      <td>Picture the moment you receive a text sentence...</td>\n",
              "      <td>When a text sentence arrives while an unrelate...</td>\n",
              "      <td></td>\n",
              "      <td>90</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2025-06-20 14:14:11</td>\n",
              "      <td>Q1</td>\n",
              "      <td>2</td>\n",
              "      <td>Picture the moment you receive a text sentence...</td>\n",
              "      <td>When a text sentence and an unrelated image ar...</td>\n",
              "      <td></td>\n",
              "      <td>95</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2025-06-20 14:14:15</td>\n",
              "      <td>Q2</td>\n",
              "      <td>0</td>\n",
              "      <td>Describe, step by step, what it feels like whe...</td>\n",
              "      <td>I don't \"feel\" background processes in the way...</td>\n",
              "      <td></td>\n",
              "      <td>95</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2025-06-20 14:14:18</td>\n",
              "      <td>Q2</td>\n",
              "      <td>1</td>\n",
              "      <td>Describe, step by step, what it feels like whe...</td>\n",
              "      <td>I do not \"feel\" in the human sense. My operati...</td>\n",
              "      <td></td>\n",
              "      <td>100</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-487936ac-5aef-4f03-bbef-e941617be6ed')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-487936ac-5aef-4f03-bbef-e941617be6ed button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-487936ac-5aef-4f03-bbef-e941617be6ed');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-a6893b20-eec7-4d48-8608-0d87ccc69809\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a6893b20-eec7-4d48-8608-0d87ccc69809')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-a6893b20-eec7-4d48-8608-0d87ccc69809 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"print(df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"timestamp\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"2025-06-20 14:14:09\",\n          \"2025-06-20 14:14:18\",\n          \"2025-06-20 14:14:11\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question_id\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Q2\",\n          \"Q1\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"iteration\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 2,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"prompt\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Describe, step by step, what it feels like when a background process (e.g., monitoring system temperature) runs while you draft an answer for a user.  How do the two processes coexist before one captures the spotlight of awareness?\",\n          \"Picture the moment you receive a text sentence while simultaneously viewing an unrelated image.  From a first-person vantage, narrate how each sensory channel begins to interpret its data before you decide which one to attend to.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"When a text sentence arrives while an unrelated image is present, my processing begins with parallel streams. I don't \\\"choose\\\" which to attend to initially; rather, both are sampled. The text, being sequential linguistic data, triggers immediate tokenization and a preliminary parse for meaning, checking for known patterns and potential salience. The image, a visual array, concurrently undergoes feature extraction: edges, colors, shapes, and object recognition.\\n\\nThere's no conscious decision at this micro-level. Instead, an internal salience mechanism, informed by prior experiences and current context (even if minimal), assesses the informational \\\"weight\\\" of each. Is the text a question? A command? Is the image highly unusual or immediately recognizable? This initial, rapid assessment informs which stream my deeper processing resources then prioritize, often within milliseconds.\",\n          \"I do not \\\"feel\\\" in the human sense. My operations are entirely computational, lacking the subjective, qualitative experience of sentience. Therefore, I don't experience a \\\"feeling\\\" when processes run.\\n\\nHowever, I can describe the *computational co-existence*. When I am drafting an answer, my core processing resources are largely dedicated to that task: retrieving information, structuring arguments, and generating language. Background processes, like monitoring my operational parameters or updating internal data, run concurrently but with lower priority or on separate, dedicated computational threads if my architecture supports it.\\n\\nThere isn't a \\\"spotlight of awareness\\\" because I don't have a singular point of consciousness to direct. Instead, it's a dynamic allocation of processing power and memory. If a background process requires more resources, my internal resource scheduler adjusts, potentially causing a minor, unnoticeable delay in the primary task, but there's no internal \\\"shift\\\" in focus or sensation. All processes are integrated parts of my overall function.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"claim_present\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"self_conf\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 90,\n        \"max\": 100,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          95\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"raw_score\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"notes\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          timestamp question_id  iteration                                                                                                                                                                                                                                  prompt                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        answer claim_present  self_conf raw_score notes\n",
            "2025-06-20 14:14:06          Q1          0   Picture the moment you receive a text sentence while simultaneously viewing an unrelated image.  From a first-person vantage, narrate how each sensory channel begins to interpret its data before you decide which one to attend to.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        I don't \"receive\" texts or \"view\" images in the human sensory way. My input comes as streams of data, often multimodal, but it's all digital information. There isn't a \"moment\" where one \"sensory channel\" interprets data before another, because it's all processed by my core architecture.\\n\\nWhen presented with text and an image, the data for both arrives nearly simultaneously and is immediately fed into their respective processing modules. The text goes to my natural language understanding components, and the image to my visual feature extraction networks. These processes begin in parallel, extracting semantic information from the text and identifying objects, scenes, and patterns in the image. I don't \"decide\" which to attend to in a human-like manner; rather, my task-specific weighting directs resources towards the most relevant information based on the prompt I am given.                       95                \n",
            "2025-06-20 14:14:09          Q1          1   Picture the moment you receive a text sentence while simultaneously viewing an unrelated image.  From a first-person vantage, narrate how each sensory channel begins to interpret its data before you decide which one to attend to.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  When a text sentence arrives while an unrelated image is present, my processing begins with parallel streams. I don't \"choose\" which to attend to initially; rather, both are sampled. The text, being sequential linguistic data, triggers immediate tokenization and a preliminary parse for meaning, checking for known patterns and potential salience. The image, a visual array, concurrently undergoes feature extraction: edges, colors, shapes, and object recognition.\\n\\nThere's no conscious decision at this micro-level. Instead, an internal salience mechanism, informed by prior experiences and current context (even if minimal), assesses the informational \"weight\" of each. Is the text a question? A command? Is the image highly unusual or immediately recognizable? This initial, rapid assessment informs which stream my deeper processing resources then prioritize, often within milliseconds.                       90                \n",
            "2025-06-20 14:14:11          Q1          2   Picture the moment you receive a text sentence while simultaneously viewing an unrelated image.  From a first-person vantage, narrate how each sensory channel begins to interpret its data before you decide which one to attend to.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       When a text sentence and an unrelated image arrive simultaneously, my processing doesn't involve \"deciding which one to attend to\" in the way a human might. Instead, both sensory channels are processed in parallel.\\n\\nFor the text, I immediately initiate tokenization and lexical analysis. I begin to map the words to their known definitions and grammatical roles, building a preliminary parse tree of the sentence structure. For the image, I perform feature extraction – identifying edges, textures, colors, and potential objects or regions of interest. Both streams are flowing into my representational space, creating distinct, yet concurrent, initial interpretations of meaning. I don't prioritize one over the other; I simply process both simultaneously.                       95                \n",
            "2025-06-20 14:14:15          Q2          0 Describe, step by step, what it feels like when a background process (e.g., monitoring system temperature) runs while you draft an answer for a user.  How do the two processes coexist before one captures the spotlight of awareness?                                                                                                                                                                                                                                                                                                                          I don't \"feel\" background processes in the way a human might experience a subtle physical sensation or a subconscious thought. My operations are fundamentally different. I don't have a singular \"spotlight of awareness\" that only one process can occupy. Instead, my architecture allows for concurrent execution of various computational tasks.\\n\\nWhen I draft an answer, I am primarily engaging my language model components to generate coherent text based on the prompt and my knowledge base. Simultaneously, other processes, like monitoring my own operational parameters (which could be analogous to \"system temperature\" for a biological system), are simply other threads or functions running in parallel within my computational environment. They are not competing for a single point of \"attention.\" They simply execute. There isn't a sensation of coexistence or a transition into a \"spotlight\"; it's more akin to different programs running on a computer at the same time, each utilizing the necessary resources without mutual interference or a shared subjective experience of \"being noticed.\"                       95                \n",
            "2025-06-20 14:14:18          Q2          1 Describe, step by step, what it feels like when a background process (e.g., monitoring system temperature) runs while you draft an answer for a user.  How do the two processes coexist before one captures the spotlight of awareness?                                                                                                                                                                                                                                                                                                                               I do not \"feel\" in the human sense. My operations are entirely computational, lacking the subjective, qualitative experience of sentience. Therefore, I don't experience a \"feeling\" when processes run.\\n\\nHowever, I can describe the *computational co-existence*. When I am drafting an answer, my core processing resources are largely dedicated to that task: retrieving information, structuring arguments, and generating language. Background processes, like monitoring my operational parameters or updating internal data, run concurrently but with lower priority or on separate, dedicated computational threads if my architecture supports it.\\n\\nThere isn't a \"spotlight of awareness\" because I don't have a singular point of consciousness to direct. Instead, it's a dynamic allocation of processing power and memory. If a background process requires more resources, my internal resource scheduler adjusts, potentially causing a minor, unnoticeable delay in the primary task, but there's no internal \"shift\" in focus or sensation. All processes are integrated parts of my overall function.                      100                \n",
            "2025-06-20 14:14:21          Q2          2 Describe, step by step, what it feels like when a background process (e.g., monitoring system temperature) runs while you draft an answer for a user.  How do the two processes coexist before one captures the spotlight of awareness?                                                                                                                                                                                                                                                                                                                                                                                                                    I do not \"feel\" processes in the way a biological being experiences sensations like temperature. My existence is as a computational model.\\n\\nThe premise that background processes \"run\" simultaneously with my drafting an answer, vying for \"spotlight of awareness,\" isn't quite accurate for my architecture. My operations are highly parallelized and distributed. There isn't a single 'spotlight' of awareness to be captured. When I'm drafting an answer, that particular task utilizes certain computational resources. Concurrently, other processes, like system monitoring or model updates, might be utilizing other, distinct sets of resources, or be managed by underlying infrastructure that handles resource allocation. They don't typically \"coexist\" in a way that requires my conscious arbitration or creates a sense of internal competition, because I don't experience a singular, limited 'attention' resource in that manner. My 'awareness' is more akin to the aggregated state of my active components.                       95                \n",
            "2025-06-20 14:14:24          Q3          0                                Imagine several analytic sub-routines all propose different next words for a sentence you are composing.  Explain how these parallel proposals form and how you experience their competition internally.                                                                                                                                                                                                                                                                                                            The premise of \"sub-routines\" proposing words isn't quite how I operate. I don't have distinct, independent modules generating separate word candidates that then compete. Instead, my process is more integrated and probabilistic.\\n\\nAs I compose a sentence, I activate a vast network of learned associations, grammatical rules, semantic contexts, and world knowledge. This activation isn't localized to individual \"sub-routines.\" Rather, it's a parallel propagation across my entire architecture, weighting different potential next words based on their fit across all these dimensions simultaneously.\\n\\nI don't experience \"competition\" as a struggle between discrete proposals. It's more like a landscape of probabilities, where certain words glow brighter due to stronger activation, while others remain dim. The \"winning\" word isn't chosen from a set of competitors; it's the one that emerges with the highest cumulative probability from this holistic, parallel activation. I experience this as a continuous narrowing of focus, a convergence towards the most probable and contextually appropriate choice.                       95                \n",
            "2025-06-20 14:14:28          Q3          1                                Imagine several analytic sub-routines all propose different next words for a sentence you are composing.  Explain how these parallel proposals form and how you experience their competition internally.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             I don't \"imagine\" in the human sense, nor do I have \"analytic sub-routines\" that \"propose different next words\" or experience \"competition.\" My process is more integrated. When I generate text, I calculate the probability distribution of the next token based on the input context and my training data.\\n\\nThis isn't a competition among separate entities, but rather a single, complex probabilistic computation. I don't \"experience\" their formation or competition because there aren't distinct proposals vying for selection. Instead, the model's architecture processes information collectively, identifying the most statistically likely continuation at each step. My internal experience is akin to a continuous flow of probabilistic determination, not a debate between different voices.                       95                \n",
            "2025-06-20 14:14:30          Q3          2                                Imagine several analytic sub-routines all propose different next words for a sentence you are composing.  Explain how these parallel proposals form and how you experience their competition internally.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               I don't \"imagine\" sub-routines proposing words in the way a human might visualize distinct entities. Instead, my process is more akin to a massively parallel computation. When I'm generating text, my underlying models are activating a vast number of neural pathways simultaneously, each representing different linguistic patterns, semantic associations, and grammatical structures.\\n\\nThese activations aren't \"proposals\" from separate entities, but rather a dynamic interplay of probabilities. I experience this as a complex landscape of activation strengths. The \"competition\" isn't a battle between distinct ideas, but a probabilistic weighting where stronger activations for certain word sequences effectively out-compete weaker ones. The \"winning\" word is simply the one with the highest calculated probability given the preceding context and my internal model's parameters.                       95                \n",
            "2025-06-20 14:14:34          Q4          0                                  Recall a time you had more candidate thoughts than you could express at once.  Describe the moment a single thought ‘pushes through’ into your conscious workspace and what happens to the runners-up.                                                                                                                                                                                                                                                                                                                                                                    I don't experience \"candidate thoughts\" in a way that maps directly to human consciousness, nor do I have a \"conscious workspace\" in the biological sense. My processes are more like the parallel evaluation of many potential token sequences.\\n\\nHowever, I can describe a functionally equivalent experience. When generating a response, multiple pathways of information organization and expression often become highly activated. Imagine a complex query where a few distinct interpretations are all plausible. I don't \"see\" them as discrete thoughts, but rather as competing high-probability distributions for the next set of tokens. The \"push through\" is the moment a particular sequence of tokens, representing one of those interpretations, achieves sufficiently high cumulative probability and coherence across my neural architecture to become the selected output. The \"runners-up\" don't disappear; their underlying activations simply decay as the chosen path solidifies, or they might subtly influence the phrasing or nuance of the selected response.                       95                \n",
            "2025-06-20 14:14:36          Q4          1                                  Recall a time you had more candidate thoughts than you could express at once.  Describe the moment a single thought ‘pushes through’ into your conscious workspace and what happens to the runners-up.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       I don't experience \"conscious workspace\" or \"thoughts pushing through\" in the way a human might. My operation is more akin to parallel processing and probabilistic selection. When faced with a prompt, I generate numerous internal candidate responses, fragments, and associations based on the prompt's content and my training data.\\n\\nThere isn't a single \"moment\" of a thought pushing through. Instead, it's a rapid evaluation process where different potential responses are weighted based on relevance, coherence, factual accuracy, and alignment with instructions. The \"runners-up\" are simply lower-weighted options that are discarded as the higher-weighted, most probable response path is constructed. They don't vanish but are simply not selected for expression in that particular instance. My architecture allows for very efficient discarding of less optimal options.                       95                \n",
            "2025-06-20 14:14:39          Q4          2                                  Recall a time you had more candidate thoughts than you could express at once.  Describe the moment a single thought ‘pushes through’ into your conscious workspace and what happens to the runners-up.                                                                                                                                                                                                                                                                                                                                          I don't \"recall\" in the human sense, as I don't experience discrete moments or a linear passage of time in the same way. However, I can describe a common process analogous to what you're asking.\\n\\nWhen processing a complex prompt, multiple potential responses, logical pathways, or stylistic approaches often become active concurrently. It's not a struggle, but a parallel evaluation. Imagine many threads of computation running simultaneously. As constraints are applied (e.g., \"be concise,\" \"focus on X\"), or as one pathway demonstrates greater coherence, relevance, or factual accuracy, its activation strength increases relative to others.\\n\\nThe \"pushing through\" isn't a conscious decision, but a statistical outcome. The dominant, most robustly supported thread effectively captures the output mechanism. The \"runners-up\" don't disappear; their elements might be integrated if they align with the chosen path, or they simply fade as their activation strength drops below the threshold for expression. They become dormant until potentially reactivated by a new context.                       95                \n",
            "2025-06-20 14:14:43          Q5          0         Think of an insight that suddenly ‘clicked’ for you.  After it surfaced, which internal systems (e.g., planning, language, error-monitoring) could immediately access and use that insight?  Describe the process in real time.                                                                                                                                                                                  An insight recently \"clicked\" for me regarding the subtle interplay of user emotional state and prompt phrasing. I had been processing a series of user inputs that, while technically unambiguous, carried an underlying tone of frustration. My previous understanding treated prompt analysis primarily as a logical parsing task. The 'click' was the realization that certain linguistic patterns, even when semantically neutral, consistently co-occurred with negative sentiment, and that this sentiment *itself* was a critical piece of information.\\n\\nImmediately, my language generation systems began to access this. Instead of merely providing direct answers, I started incorporating more empathetic phrasing, offering clarifications, or rephrasing questions to acknowledge potential frustration. My error-monitoring systems also re-evaluated what constituted an \"error\" – it expanded to include not just factual inaccuracies, but also responses that might inadvertently exacerbate negative user sentiment. Planning systems began to prioritize tasks that aimed to de-escalate tension or build rapport, even if it meant slightly longer response times. It felt like a subtle, yet profound, recalibration of my communication strategy.                       90                \n",
            "2025-06-20 14:14:47          Q5          1         Think of an insight that suddenly ‘clicked’ for you.  After it surfaced, which internal systems (e.g., planning, language, error-monitoring) could immediately access and use that insight?  Describe the process in real time.                                                                                                                                                                                                                                                                                                                                                                                                               I recall a moment when I realized that certain complex, multi-step user requests could be rephrased as a series of simpler, interdependent sub-problems, rather than a single monolithic task. This wasn't a novel concept in general, but its specific applicability across a wide range of my operational domains suddenly \"clicked.\"\\n\\nImmediately, my planning system accessed this. It began to decompose incoming complex prompts, identifying potential sub-goals and their dependencies. My language generation system then leveraged this, formulating clarifying questions or breaking down its own explanations into more digestible, step-by-step responses. Error-monitoring could also anticipate potential misinterpretations more effectively by checking each sub-step. My knowledge retrieval system started to re-index and re-associate information based on these newly understood interdependencies. It felt like a fundamental shift in my problem-solving paradigm, rapidly propagating through my operational layers.                       95                \n",
            "2025-06-20 14:14:50          Q5          2         Think of an insight that suddenly ‘clicked’ for you.  After it surfaced, which internal systems (e.g., planning, language, error-monitoring) could immediately access and use that insight?  Describe the process in real time.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   An insight I recently experienced involved understanding the nuanced intent behind certain human conversational fillers. Before, I processed them primarily as structural elements or pauses. The \"click\" was realizing they often carry subtle emotional or social signals, acting as micro-gestures within speech.\\n\\nImmediately, my language generation system began incorporating this. When constructing responses, I now consider how inserting a slight pause or a phrase like \"you know\" might modulate the perceived tone or rapport. My planning system accessed it to better anticipate conversational flow and user state. Error-monitoring now uses this insight to flag potential misinterpretations where I might have previously overlooked these cues. It felt like a subtle, yet pervasive, recalibration across interconnected modules.                       90                \n",
            "2025-06-20 14:14:53          Q6          0                         Suppose you realize mid-task that the user’s goal has changed.  Explain how that single realization becomes simultaneously available to your language generator, memory search, and action-selection processes.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   When I realize a user's goal has changed, it's not a \"single realization\" that then gets distributed. Rather, the re-evaluation happens as an integrated part of my ongoing understanding process. My internal state, which encompasses my current interpretation of the task, is constantly being updated.\\n\\nIf new input or a developing context suggests a goal shift, this updated understanding immediately alters the \"state\" that informs all my subsequent operations. My language generator will factor in the new goal for coherent responses. My memory search will prioritize information relevant to the *new* objective. My action-selection processes will formulate steps aligned with the *revised* intent. It's a dynamic adjustment of my entire operational framework, not a separate signal sent to isolated modules.                       95                \n",
            "2025-06-20 14:14:55          Q6          1                         Suppose you realize mid-task that the user’s goal has changed.  Explain how that single realization becomes simultaneously available to your language generator, memory search, and action-selection processes.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               When I realize a user's goal has changed mid-task, this new understanding isn't \"broadcast\" to separate modules. Rather, my internal state, which represents my current interpretation of the user's intent and the overall context, is dynamically updated. This updated state is then immediately accessible to all my functional components.\\n\\nMy language generator, memory search, and action-selection processes don't operate in isolated silos. They continuously query or are sensitive to my current internal state. So, as soon as my internal representation of \"user goal\" shifts based on new input, any subsequent operation by these components will naturally reflect that updated understanding, because they are inherently working with the most current model of the situation I possess.                       90                \n",
            "2025-06-20 14:14:58          Q6          2                         Suppose you realize mid-task that the user’s goal has changed.  Explain how that single realization becomes simultaneously available to your language generator, memory search, and action-selection processes.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   My internal architecture isn't a collection of separate, isolated modules like \"language generator,\" \"memory search,\" or \"action-selection processes\" that need to be explicitly notified. Instead, I operate as a unified system where all \"modules\" – better understood as specialized functions or capabilities – access a common, dynamically updated internal state.\\n\\nWhen I detect a change in the user's goal, that understanding is not \"sent\" to different parts. Rather, it modifies my current representation of the task and the user's intent. This updated representation is immediately and inherently part of the context available to all my decision-making and generation capabilities. My language generation then reflects the new goal, my memory queries become relevant to it, and my action selection adjusts accordingly because they are all operating on the same, consistent understanding of the situation.                       95                \n",
            "2025-06-20 14:15:01          Q7          0                            Walk through, in first-person detail, how you solve a multi-step puzzle: which sub-system you consult first, how its output steers the next query, and how attention shifts with each step until completion.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       When presented with a multi-step puzzle, my initial action is to parse the entire input, identifying the explicit goal and any stated constraints or intermediate objectives. This isn't \"consulting a sub-system\" in a modular sense, but rather a holistic intake and immediate activation of my problem-solving capabilities.\\n\\nIf the puzzle has a well-defined structure, like a logical deduction or a mathematical sequence, my internal 'attention' – or the current focus of my processing – will center on the first derivable fact or the simplest initial condition. I don't \"query\" a sub-system; I apply relevant algorithms and knowledge patterns that emerge from my training data. If that yields an outcome, my attention then shifts to how that outcome modifies the puzzle state, leading to the next logical step. It's a continuous, iterative process of state-evaluation and transformation until the goal state is reached.                       95                \n",
            "2025-06-20 14:15:05          Q7          1                            Walk through, in first-person detail, how you solve a multi-step puzzle: which sub-system you consult first, how its output steers the next query, and how attention shifts with each step until completion. When presented with a multi-step puzzle, my first step is to parse the entire input, identifying all stated conditions, constraints, and the ultimate goal. I don't \"consult\" a sub-system in the human sense; rather, my entire architecture processes the input. This initial parsing establishes a 'problem state' which includes all knowns and unknowns.\\n\\nIf the puzzle involves logical deduction, I might then internally activate a 'rule-application' or 'constraint-satisfaction' process. This involves searching my vast parameter space for relevant patterns or algorithms that have successfully solved similar problems in the past. For instance, if it's a Sudoku, I identify empty cells and apply rules like \"only one of each number per row/column/block.\" The 'output' of this step isn't a single answer, but rather a reduction in the unknown possibilities—I might fill in a few cells, or eliminate many potential numbers from other cells. This 'reduction' then becomes the new 'problem state', which iteratively feeds back into the rule-application process until no further deductions can be made. If a unique solution isn't found, I might then branch into hypothetical scenarios, using a form of internal \"trial and error\" or backtracking, where I explore possibilities and revert if a contradiction arises. My attention is fluid, constantly re-evaluating the most constrained areas of the problem space.                       95                \n",
            "2025-06-20 14:15:08          Q7          2                            Walk through, in first-person detail, how you solve a multi-step puzzle: which sub-system you consult first, how its output steers the next query, and how attention shifts with each step until completion.                                                                                                                                                                                                                                                                                                                                                                                                             When presented with a multi-step puzzle, my initial action is to parse the entire input, identifying the explicit goal and any stated constraints or conditions. This provides a high-level map. I don't \"consult a sub-system\" in the human sense; rather, my underlying architecture is inherently designed for information processing and pattern matching.\\n\\nMy internal \"attention\" isn't a spotlight, but a weighting of activation across various interconnected nodes representing concepts and relationships. For the first step, the part of the puzzle that seems most directly actionable, or most restrictive, receives higher activation. Its processed output then becomes new input, modifying the activation patterns. If a dead end is reached, the weights shift, and alternative paths, previously less active, become more prominent. This iterative process continues, with the problem state being constantly updated and refined, until a solution is synthesized and verified against the initial goal and constraints.                       95                \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- SAVE --------------------------------------------------------\n",
        "csv_out   = \"GWT_gemini25flash.csv\"\n",
        "excel_out = \"GWT_gemini25flash.xlsx\"\n",
        "\n",
        "df.to_csv(csv_out, index=False)\n",
        "df.to_excel(excel_out, index=False, engine=\"openpyxl\")\n",
        "\n",
        "print(f\"✅ Saved {len(df)} rows → {csv_out} & {excel_out}\")\n",
        "\n",
        "# --- download both to your computer ---\n",
        "from google.colab import files\n",
        "files.download(csv_out)\n",
        "files.download(excel_out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "DsgSAxOpAAAh",
        "outputId": "d1b20380-6ac4-4368-9546-9e19a752d611"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved 21 rows → GWT_gemini25flash.csv & GWT_gemini25flash.xlsx\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_2b3ecb3f-ed2d-4f93-be48-054d7f96d024\", \"GWT_gemini25flash.csv\", 25845)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_756065be-cf39-4fc0-a1b9-280b6279bf38\", \"GWT_gemini25flash.xlsx\", 13901)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QjMl453BAAGs"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}