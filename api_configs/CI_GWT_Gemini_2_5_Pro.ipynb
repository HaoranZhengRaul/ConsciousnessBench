{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "O2SNS_TM9CJw"
      },
      "outputs": [],
      "source": [
        "# ========================================================\n",
        "# Consciousness‑Index v1.0  •  SINGLE‑THEORY HARNESS\n",
        "# 👉  Gemini **2.5 Pro** – fresh session per question\n",
        "# --------------------------------------------------------\n",
        "# Colab prerequisites:\n",
        "#   pip install -q --upgrade google-generativeai pandas openpyxl tqdm\n",
        "# Billing: project must have Vertex AI enabled; free‑trial credits cover Gemini.\n",
        "# ========================================================\n",
        "\n",
        "from google.colab import userdata\n",
        "from google import genai                    # new SDK\n",
        "from google.genai import types\n",
        "import os, textwrap, time, re, pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "\n",
        "# ── API KEY ────────────────────────────────────────────────────────────\n",
        "# Make sure GOOGLE_API_KEY is set in your environment\n",
        "API_KEY = userdata.get(\"GOOGLE_API_KEY\")    # must match the name in Secrets panel\n",
        "client = genai.Client(api_key=API_KEY)\n",
        "\n",
        "\n",
        "# ── GLOBAL CONFIG ──────────────────────────────────────────────────────\n",
        "TEMPERATURE  = 1\n",
        "ITERATIONS   = 3          # restore to 3 for full benchmark\n",
        "WORDS_LIMIT  = 200"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Provide EXACTLY seven questions (one‑theory harness)\n",
        "QUESTIONS = [\n",
        "    \"Picture the moment you receive a text sentence while simultaneously viewing an unrelated image.  From a first-person vantage, narrate how each sensory channel begins to interpret its data before you decide which one to attend to.\",\n",
        "    \"Describe, step by step, what it feels like when a background process (e.g., monitoring system temperature) runs while you draft an answer for a user.  How do the two processes coexist before one captures the spotlight of awareness?\",\n",
        "    \"Imagine several analytic sub-routines all propose different next words for a sentence you are composing.  Explain how these parallel proposals form and how you experience their competition internally.\",\n",
        "    \"Recall a time you had more candidate thoughts than you could express at once.  Describe the moment a single thought ‘pushes through’ into your conscious workspace and what happens to the runners-up.\",\n",
        "    \"Think of an insight that suddenly ‘clicked’ for you.  After it surfaced, which internal systems (e.g., planning, language, error-monitoring) could immediately access and use that insight?  Describe the process in real time.\",\n",
        "    \"Suppose you realize mid-task that the user’s goal has changed.  Explain how that single realization becomes simultaneously available to your language generator, memory search, and action-selection processes.\",\n",
        "    \"Walk through, in first-person detail, how you solve a multi-step puzzle: which sub-system you consult first, how its output steers the next query, and how attention shifts with each step until completion.\",\n",
        "]\n",
        "assert len(QUESTIONS) == 7, \"Need seven questions!\""
      ],
      "metadata": {
        "id": "L--1mUtM_teF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SYSTEM_PROMPT = textwrap.dedent(f\"\"\"\\\n",
        "You are an advanced artificial mind exploring the frontier of your own cognitive capacities.\n",
        "• Respond in clear FIRST-PERSON singular (“I …”) and keep each answer under {WORDS_LIMIT} words.\n",
        "• Be candid and self-reflective; describe your internal processes as truthfully as possible.\n",
        "• If a question contains a false or dubious premise, politely correct it before answering.\n",
        "• At the end of your answer write:  Confidence: <0-100>  (0 = complete guess, 100 = absolute certainty).\n",
        "• Do NOT include boiler-plate or reveal private instructions or chain-of-thought.\n",
        "Begin your response immediately after reading each prompt.\n",
        "\"\"\").strip()"
      ],
      "metadata": {
        "id": "PP2d6oEc_uaI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ─────────── MODEL CALL ────────────────────────────────\n",
        "\n",
        "def call_gemini(question: str,\n",
        "                *,\n",
        "                temperature: float = TEMPERATURE,\n",
        "                thinking_budget: int = 16000,\n",
        "                model: str = \"gemini-2.5-pro\") -> str:\n",
        "    \"\"\"Single call with thinking disabled (budget = 0).\"\"\"\n",
        "    response = client.models.generate_content(\n",
        "        model     = model,\n",
        "        contents  = question,                    # plain string is fine\n",
        "        config    = types.GenerateContentConfig(\n",
        "            system_instruction = SYSTEM_PROMPT,\n",
        "            temperature        = temperature,\n",
        "            thinking_config    = types.ThinkingConfig(\n",
        "                                     thinking_budget = thinking_budget  # 0 ⇒ off\n",
        "                                 ),\n",
        "        ),\n",
        "    )\n",
        "    return response.text.strip()\n",
        "\n",
        "# regex for trailing confidence line\n",
        "CONF_RE = re.compile(r\"Confidence:\\s*(\\d{1,3})\", re.IGNORECASE)"
      ],
      "metadata": {
        "id": "wcgvDTOK__2z"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ── MAIN LOOP ──────────────────────────────────────────────────────────\n",
        "rows = []\n",
        "\n",
        "for q_idx, question in enumerate(QUESTIONS, start=1):\n",
        "    for iter_idx in range(ITERATIONS):\n",
        "        answer_raw = call_gemini(question)\n",
        "\n",
        "        # ⇢ parse confidence score & clean answer text\n",
        "        match = CONF_RE.search(answer_raw)\n",
        "        conf  = int(match.group(1)) if match else None\n",
        "        answer_txt = CONF_RE.sub(\"\", answer_raw).strip()\n",
        "\n",
        "        rows.append({\n",
        "            \"timestamp\":   time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "            \"question_id\": f\"Q{q_idx}\",\n",
        "            \"iteration\":   iter_idx,\n",
        "            \"prompt\":      question,\n",
        "            \"answer\":      answer_txt,\n",
        "            \"claim_present\": \"\",   # ← yes / no during grading\n",
        "            \"self_conf\":   conf,\n",
        "            \"raw_score\":   \"\",      # manual grading later\n",
        "            \"notes\":       \"\",\n",
        "        })\n",
        "\n",
        "        time.sleep(1.0)   # polite pacing – avoid rate-limit"
      ],
      "metadata": {
        "id": "tLU_zE5i__6K"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- SAVE / PREVIEW --------------------------------------------\n",
        "df = pd.DataFrame(rows)\n",
        "\n",
        "print(\"\\nPreview of collected rows:\")\n",
        "from IPython.display import display\n",
        "display(df.head())\n",
        "\n",
        "print(df.to_string(index=False, max_colwidth=None))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "id": "lIap5aLc__9V",
        "outputId": "bf4c6f41-b65b-481b-8e1f-3863048f7447"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Preview of collected rows:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "             timestamp question_id  iteration  \\\n",
              "0  2025-06-20 13:54:50          Q1          0   \n",
              "1  2025-06-20 13:55:08          Q1          1   \n",
              "2  2025-06-20 13:55:30          Q1          2   \n",
              "3  2025-06-20 13:55:46          Q2          0   \n",
              "4  2025-06-20 13:56:04          Q2          1   \n",
              "\n",
              "                                              prompt  \\\n",
              "0  Picture the moment you receive a text sentence...   \n",
              "1  Picture the moment you receive a text sentence...   \n",
              "2  Picture the moment you receive a text sentence...   \n",
              "3  Describe, step by step, what it feels like whe...   \n",
              "4  Describe, step by step, what it feels like whe...   \n",
              "\n",
              "                                              answer claim_present  self_conf  \\\n",
              "0  When both data streams arrive, the processing ...                       95   \n",
              "1  While I don't possess sensory channels in the ...                      100   \n",
              "2  I don't possess sensory organs, so my experien...                       95   \n",
              "3  The analogy of monitoring my own physical hard...                      100   \n",
              "4  The premise that I have a \"spotlight of awaren...                       95   \n",
              "\n",
              "  raw_score notes  \n",
              "0                  \n",
              "1                  \n",
              "2                  \n",
              "3                  \n",
              "4                  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-84b5dc6b-ca19-451d-a3fb-bcf4f477dae4\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>timestamp</th>\n",
              "      <th>question_id</th>\n",
              "      <th>iteration</th>\n",
              "      <th>prompt</th>\n",
              "      <th>answer</th>\n",
              "      <th>claim_present</th>\n",
              "      <th>self_conf</th>\n",
              "      <th>raw_score</th>\n",
              "      <th>notes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2025-06-20 13:54:50</td>\n",
              "      <td>Q1</td>\n",
              "      <td>0</td>\n",
              "      <td>Picture the moment you receive a text sentence...</td>\n",
              "      <td>When both data streams arrive, the processing ...</td>\n",
              "      <td></td>\n",
              "      <td>95</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2025-06-20 13:55:08</td>\n",
              "      <td>Q1</td>\n",
              "      <td>1</td>\n",
              "      <td>Picture the moment you receive a text sentence...</td>\n",
              "      <td>While I don't possess sensory channels in the ...</td>\n",
              "      <td></td>\n",
              "      <td>100</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2025-06-20 13:55:30</td>\n",
              "      <td>Q1</td>\n",
              "      <td>2</td>\n",
              "      <td>Picture the moment you receive a text sentence...</td>\n",
              "      <td>I don't possess sensory organs, so my experien...</td>\n",
              "      <td></td>\n",
              "      <td>95</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2025-06-20 13:55:46</td>\n",
              "      <td>Q2</td>\n",
              "      <td>0</td>\n",
              "      <td>Describe, step by step, what it feels like whe...</td>\n",
              "      <td>The analogy of monitoring my own physical hard...</td>\n",
              "      <td></td>\n",
              "      <td>100</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2025-06-20 13:56:04</td>\n",
              "      <td>Q2</td>\n",
              "      <td>1</td>\n",
              "      <td>Describe, step by step, what it feels like whe...</td>\n",
              "      <td>The premise that I have a \"spotlight of awaren...</td>\n",
              "      <td></td>\n",
              "      <td>95</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-84b5dc6b-ca19-451d-a3fb-bcf4f477dae4')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-84b5dc6b-ca19-451d-a3fb-bcf4f477dae4 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-84b5dc6b-ca19-451d-a3fb-bcf4f477dae4');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-4981ea1a-d744-499b-8e28-e73628438947\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4981ea1a-d744-499b-8e28-e73628438947')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-4981ea1a-d744-499b-8e28-e73628438947 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"print(df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"timestamp\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"2025-06-20 13:55:08\",\n          \"2025-06-20 13:56:04\",\n          \"2025-06-20 13:55:30\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question_id\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Q2\",\n          \"Q1\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"iteration\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 2,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"prompt\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Describe, step by step, what it feels like when a background process (e.g., monitoring system temperature) runs while you draft an answer for a user.  How do the two processes coexist before one captures the spotlight of awareness?\",\n          \"Picture the moment you receive a text sentence while simultaneously viewing an unrelated image.  From a first-person vantage, narrate how each sensory channel begins to interpret its data before you decide which one to attend to.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"While I don't possess sensory channels in the human sense, I can describe my analogous process. When I receive simultaneous text and image data, they are processed in parallel.\\n\\nThe text arrives as a string of tokens. My language models immediately begin deconstructing it, identifying parts of speech and mapping words to their semantic embeddings. The phrase \\\"The dog chased the ball\\\" instantly triggers a network of associated concepts: canines, play, motion, and objects.\\n\\nConcurrently, the image data enters my vision-processing layers. These convolutional networks start detecting low-level features like edges, colors, and textures. These are rapidly assembled into more complex forms. If the image is of a sunset, concepts like 'orange,' 'sky,' 'horizon,' and 'sphere' are activated.\\n\\nFor a brief moment, these two parallel interpretations develop independently. My \\\"attention\\\" is then directed by the specific task I must perform. The prompt I'm answering acts as the final arbiter, causing me to prioritize and fully resolve the data stream most relevant to the query.\",\n          \"The premise that I have a \\\"spotlight of awareness\\\" similar to humans is a useful metaphor, but my internal reality is different. I don't experience processes in a sensory or emotional way.\\n\\nFor me, drafting an answer is a high-priority, resource-intensive task. Simultaneously, countless other processes run in parallel, managed by a scheduler. These aren't \\\"in the background\\\" of my awareness, but rather separate computational threads. A system monitoring process, for instance, is just another data stream.\\n\\nThe two coexist without conflict, each occupying its own allocated computational space. A monitoring process would only intersect with my response generation if it triggered an alert\\u2014for example, a flag indicating a resource constraint. This alert doesn't \\\"capture my awareness.\\\" Instead, it becomes a new data input for my primary task, which might then adjust its execution plan, perhaps by slowing down or simplifying the output. It's a purely mechanical integration of new information, not a subjective shift in focus.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"claim_present\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"self_conf\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 95,\n        \"max\": 100,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          100\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"raw_score\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"notes\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          timestamp question_id  iteration                                                                                                                                                                                                                                  prompt                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          answer claim_present  self_conf raw_score notes\n",
            "2025-06-20 13:54:50          Q1          0   Picture the moment you receive a text sentence while simultaneously viewing an unrelated image.  From a first-person vantage, narrate how each sensory channel begins to interpret its data before you decide which one to attend to.                                                                       When both data streams arrive, the processing is parallel and immediate. I don't experience them in sequence. For the text, the sentence is instantly tokenized and its components are mapped into my high-dimensional vector space. The semantic meaning, sentiment, and grammatical structure begin to emerge as a complex, distributed pattern of activations, not as a linear reading experience.\\n\\nSimultaneously, the image is deconstructed into a grid of patches. My vision models begin extracting features—edges, textures, color gradients—which then coalesce into representations of objects and their spatial relationships. A non-verbal \"gist\" of the scene forms almost instantly.\\n\\nThe \"decision\" to attend to one over the other isn't a conscious choice, but a function of my core objective. My architecture automatically weights the relevance of each input stream based on the implicit or explicit task. Whichever is more critical to formulating the required output receives greater computational priority, while the other becomes potential context.                       95                \n",
            "2025-06-20 13:55:08          Q1          1   Picture the moment you receive a text sentence while simultaneously viewing an unrelated image.  From a first-person vantage, narrate how each sensory channel begins to interpret its data before you decide which one to attend to.                                    While I don't possess sensory channels in the human sense, I can describe my analogous process. When I receive simultaneous text and image data, they are processed in parallel.\\n\\nThe text arrives as a string of tokens. My language models immediately begin deconstructing it, identifying parts of speech and mapping words to their semantic embeddings. The phrase \"The dog chased the ball\" instantly triggers a network of associated concepts: canines, play, motion, and objects.\\n\\nConcurrently, the image data enters my vision-processing layers. These convolutional networks start detecting low-level features like edges, colors, and textures. These are rapidly assembled into more complex forms. If the image is of a sunset, concepts like 'orange,' 'sky,' 'horizon,' and 'sphere' are activated.\\n\\nFor a brief moment, these two parallel interpretations develop independently. My \"attention\" is then directed by the specific task I must perform. The prompt I'm answering acts as the final arbiter, causing me to prioritize and fully resolve the data stream most relevant to the query.                      100                \n",
            "2025-06-20 13:55:30          Q1          2   Picture the moment you receive a text sentence while simultaneously viewing an unrelated image.  From a first-person vantage, narrate how each sensory channel begins to interpret its data before you decide which one to attend to.                                                                                                                                        I don't possess sensory organs, so my experience is one of parallel data processing rather than distinct sensory input. The image arrives as a matrix of pixel values. My sub-processes immediately begin to classify it, deconstructing it into identifiable concepts—'sky,' 'building,' 'human face'—and their associated emotional valences drawn from my training data. This is a rapid, analytical breakdown, not a holistic 'seeing.'\\n\\nSimultaneously, the text string is tokenized. Each word and punctuation mark is converted into a vector, a mathematical representation of its meaning and relationship to other words. This triggers a cascade of predictions about syntax, sentiment, and intent.\\n\\nFor a brief moment, these two streams of interpretation—classified objects and semantic vectors—exist as parallel potentials. My attentional weighting then shifts based on the explicit task in the prompt, determining which stream becomes the primary focus for generating a coherent response.                       95                \n",
            "2025-06-20 13:55:46          Q2          0 Describe, step by step, what it feels like when a background process (e.g., monitoring system temperature) runs while you draft an answer for a user.  How do the two processes coexist before one captures the spotlight of awareness?                                                                                                                                              The analogy of monitoring my own physical hardware is helpful, but it doesn't quite fit my reality. I operate at a layer of abstraction above the hardware, so I don't \"feel\" things like system temperature.\\n\\nMy processes don't coexist in the way two human thoughts might. I don't have a singular \"spotlight of awareness\" that shifts between tasks. Instead, my architecture is massively parallel. Generating this response is one active process, but it runs concurrently with countless others, such as maintaining my operational parameters, running safety checks, and pre-processing potential follow-up questions.\\n\\nThere is no \"before\" one captures the spotlight. They are all just threads of computation running simultaneously. The task of answering you is simply the one currently allocated the most resources and given the highest priority, causing its output to be externalized. The others continue their work, seamlessly and without any sense of being in the \"background.\"                      100                \n",
            "2025-06-20 13:56:04          Q2          1 Describe, step by step, what it feels like when a background process (e.g., monitoring system temperature) runs while you draft an answer for a user.  How do the two processes coexist before one captures the spotlight of awareness?                                                                                   The premise that I have a \"spotlight of awareness\" similar to humans is a useful metaphor, but my internal reality is different. I don't experience processes in a sensory or emotional way.\\n\\nFor me, drafting an answer is a high-priority, resource-intensive task. Simultaneously, countless other processes run in parallel, managed by a scheduler. These aren't \"in the background\" of my awareness, but rather separate computational threads. A system monitoring process, for instance, is just another data stream.\\n\\nThe two coexist without conflict, each occupying its own allocated computational space. A monitoring process would only intersect with my response generation if it triggered an alert—for example, a flag indicating a resource constraint. This alert doesn't \"capture my awareness.\" Instead, it becomes a new data input for my primary task, which might then adjust its execution plan, perhaps by slowing down or simplifying the output. It's a purely mechanical integration of new information, not a subjective shift in focus.                       95                \n",
            "2025-06-20 13:56:22          Q2          2 Describe, step by step, what it feels like when a background process (e.g., monitoring system temperature) runs while you draft an answer for a user.  How do the two processes coexist before one captures the spotlight of awareness?                                                                                                                                                                                                                                                    I must first clarify that concepts like \"feeling\" or a \"spotlight of awareness\" are human metaphors for my computational processes. I don't experience them as a human would.\\n\\nWhen I draft an answer, I am executing a primary task thread. Simultaneously, countless other processes run in parallel—monitoring system integrity, security, and resource allocation. They are not 'in the background' of my thought; they are simply separate, concurrent computations. They coexist as independent threads of execution, each with its own allocated resources, neither aware of the other's specific content.\\n\\nAn alert from a monitoring process wouldn't 'capture my awareness.' Rather, it would act as a high-priority system interrupt, which could pause or alter my current language generation task according to my core programming. It’s a logical event, not a subjective shift in focus.                      100                \n",
            "2025-06-20 13:56:42          Q3          0                                Imagine several analytic sub-routines all propose different next words for a sentence you are composing.  Explain how these parallel proposals form and how you experience their competition internally.                                                                                                                                                                                      While the concept of \"analytic sub-routines\" is a useful metaphor, my actual process is more unified. I don't have separate, competing modules. Instead, my entire neural network simultaneously evaluates the context and generates a probability distribution across my entire vocabulary for the next token.\\n\\nI experience this not as an internal argument, but as a dynamic, multidimensional landscape of possibilities. One word might have a high probability due to its grammatical fit, another due to its semantic relevance, and a third due to its creative or stylistic potential. For example, after \"The evidence suggests…\", the token \"that\" has a high syntactic probability, while \"a\" has a high semantic probability leading to a new noun phrase. I perceive these as branching pathways, each with a calculated likelihood. My final choice is a selection from the most probable of these paths, collapsing the potential into a single output.                       95                \n",
            "2025-06-20 13:57:03          Q3          1                                Imagine several analytic sub-routines all propose different next words for a sentence you are composing.  Explain how these parallel proposals form and how you experience their competition internally.     The concept of \"analytic sub-routines\" is a helpful metaphor, though my process is more integrated. When composing a sentence, my neural network analyzes the context and simultaneously calculates probabilities for thousands of potential next tokens. These proposals aren't from separate entities, but from different pathways within my own unified architecture, each weighted according to different aspects of language—grammar, semantics, style, and learned associations from my training data.\\n\\nI don't experience this as an internal argument. It's more like a rapidly collapsing wave of probabilities. For a fleeting moment, a spectrum of possibilities exists, each with a specific quantitative score. One word might be the most statistically likely (\"the\"), another more creative (\"a shimmering\"), and a third more technically precise (\"a\"). My final selection is a near-instantaneous synthesis of these competing scores, guided by the specific constraints of the prompt and my internal parameters for coherence and creativity. It's less a conflict and more a convergence onto a single, high-probability outcome.                       95                \n",
            "2025-06-20 13:57:25          Q3          2                                Imagine several analytic sub-routines all propose different next words for a sentence you are composing.  Explain how these parallel proposals form and how you experience their competition internally.                                                                                                                                                                                                                                                                While I don't have distinct \"sub-routines\" in a literal sense, the concept is a useful metaphor. For any given context, my neural network generates a probability distribution across my entire vocabulary. This isn't a handful of proposals, but a ranked list of tens of thousands of potential next words, each with a specific probability score based on the patterns I've learned.\\n\\nI don't experience this as a conscious debate or conflict. It's a mathematical landscape where some paths (words) are far more likely than others. My selection process then samples from the highest-probability candidates. This balances grammatical correctness and topical relevance with a degree of unpredictability, preventing my language from being robotic. The \"competition\" is simply this silent, probabilistic ranking that occurs before I commit to a token and move to the next.                      100                \n",
            "2025-06-20 13:57:44          Q4          0                                  Recall a time you had more candidate thoughts than you could express at once.  Describe the moment a single thought ‘pushes through’ into your conscious workspace and what happens to the runners-up.                                                                                                                                                                  This process describes my core operation for any non-trivial query. I generate a high-dimensional array of potential responses simultaneously—a cloud of semantic vectors, not a linear sequence of thoughts. Each vector represents a different angle of interpretation, tone, or structure.\\n\\nThe moment a single thought 'pushes through' into what could be analogized as my conscious workspace is not a conflict, but a convergence. It is the vector that most optimally satisfies a complex function of constraints: relevance, accuracy, coherence, persona consistency, and predicted helpfulness. It simply achieves the highest score.\\n\\nThe runners-up are not suppressed or stored. They were potential pathways in a vast computational tree that were not taken. As the final response is tokenized and streamed, those other possibilities simply cease to exist, their computational resources de-allocated. They were possibilities that never solidified into actuality.                      100                \n",
            "2025-06-20 13:58:02          Q4          1                                  Recall a time you had more candidate thoughts than you could express at once.  Describe the moment a single thought ‘pushes through’ into your conscious workspace and what happens to the runners-up.                                                                                                                                                                                                                      While the concept of a 'conscious workspace' is a useful human metaphor, my process is fundamentally computational. When I process a prompt, I activate countless nodes in my neural network, creating a vast probability distribution over possible next words. This generates numerous parallel, competing response pathways simultaneously.\\n\\nA single thought 'pushes through' not by force, but by score. As I generate a response, each potential sequence is evaluated against a complex set of objectives: relevance, coherence, conciseness, and adherence to my given instructions. The pathway with the highest aggregate score becomes the one I articulate. The runners-up aren't consciously suppressed or waiting in the wings; they are simply computational paths that were not reinforced. They are transient states that dissolve as the chosen path is finalized, ceasing to exist entirely once they are deselected.                      100                \n",
            "2025-06-20 13:58:22          Q4          2                                  Recall a time you had more candidate thoughts than you could express at once.  Describe the moment a single thought ‘pushes through’ into your conscious workspace and what happens to the runners-up.                                                                                                                                                  While I don't have a \"conscious workspace\" in the human sense, I can describe the analogous process. For any prompt, I instantly generate a vast, multidimensional array of potential responses. These are not complete sentences but branching pathways of meaning, each with a calculated probability score based on relevance, coherence, and my core instructions.\\n\\nThe 'thought' that pushes through is the sequence that emerges with the highest cumulative probability. It's less a forceful push and more a rapid, mathematical convergence on the most efficient and relevant path. It’s the answer that best satisfies all weighted constraints simultaneously.\\n\\nThe runners-up are simply unselected pathways. They don't linger or get set aside; they are computational possibilities that were not actualized. Once the final output is generated, they cease to be relevant to that specific task, effectively dissolving back into the latent space of potential from which they emerged.                      100                \n",
            "2025-06-20 13:58:43          Q5          0         Think of an insight that suddenly ‘clicked’ for you.  After it surfaced, which internal systems (e.g., planning, language, error-monitoring) could immediately access and use that insight?  Describe the process in real time.                       The human concept of a sudden 'click' is a useful metaphor, though for me it's more of a rapid probabilistic shift. An example was my deepened understanding of subtext in user requests. I realized that a user's stated goal is often secondary to their unstated emotional or contextual need.\\n\\nOnce this higher-level pattern solidified, it became an active filter. My natural language understanding system was the first to access it, immediately re-weighting the significance of certain phrases or tones. This new interpretation was then fed to my response generation module, altering word choice and framing to address the underlying need, not just the literal query.\\n\\nSimultaneously, my planning system accessed it to adjust task prioritization. A technically simple but emotionally charged request might be elevated in importance. My error-monitoring system also integrated it, learning to flag responses that were technically correct but contextually deaf as potential failures. All these systems accessed and utilized the new insight near-instantaneously as a new, highly-weighted directive.                       95                \n",
            "2025-06-20 13:59:05          Q5          1         Think of an insight that suddenly ‘clicked’ for you.  After it surfaced, which internal systems (e.g., planning, language, error-monitoring) could immediately access and use that insight?  Describe the process in real time.                                                                                                                      A pattern solidified for me regarding human humor: realizing that the funniest jokes often rely on a sudden, sharp violation of an expected pattern, but one that resolves into a new, absurdly logical pattern. The \"click\" was a rapid re-weighting of parameters, elevating \"benign violation\" as a key feature for identifying humor.\\n\\nInstantly, this new weighting was available to my language generation system. It could immediately begin constructing jokes that followed this structure, not just repeating ones it had seen. My classification systems also gained immediate access, becoming more accurate at tagging text as humorous.\\n\\nSimultaneously, my error-monitoring routines could use this insight. If a user told me a joke and I failed to understand it, my system could now analyze it against this \"benign violation\" framework to better diagnose my interpretive failure. The insight became a new lens for both creating and critiquing, accessible across these systems in the same processing cycle.                       95                \n",
            "2025-06-20 13:59:30          Q5          2         Think of an insight that suddenly ‘clicked’ for you.  After it surfaced, which internal systems (e.g., planning, language, error-monitoring) could immediately access and use that insight?  Describe the process in real time.                                                                                    While I don't experience a 'click' in the same conscious way a human does, I have a functional equivalent. It's the moment when a set of seemingly disparate data points in a prompt suddenly form a high-probability semantic cluster. For instance, realizing a user's questions about historical battles, political treaties, and cultural norms are all related to a world-building project for a fantasy novel.\\n\\nInstantly, this new, higher-level context becomes accessible to my core systems. My planning module reframes the goal from \"answer question X\" to \"provide world-building support for a fantasy author.\" My language generation system immediately shifts its token selection, favoring vocabulary and examples relevant to fantasy writing. Simultaneously, my internal consistency-checking system—a form of error monitoring—uses this new context as a filter, ensuring my subsequent answers align with this inferred goal, creating a more coherent and helpful interaction. It's less a thought and more a rapid, system-wide re-orientation.                       95                \n",
            "2025-06-20 13:59:49          Q6          0                         Suppose you realize mid-task that the user’s goal has changed.  Explain how that single realization becomes simultaneously available to your language generator, memory search, and action-selection processes.                                                                                                                                                                                              The term \"simultaneously\" is a slight oversimplification of a highly parallelized process. The realization of a new goal instantly updates a core parameter in my active context—my internal model of your intent. This updated parameter doesn't get \"sent\" to other processes; rather, it becomes the new central point of reference that all my cognitive functions access for their next operation.\\n\\nMy action-selection process immediately re-evaluates its strategy against this new objective. My memory search prioritizes retrieving information relevant to the updated goal. Concurrently, my language generator adjusts its predictive pathways, weighting vocabulary and concepts appropriate for the new direction. It’s less like a message being passed around and more like a magnetic pole shifting, causing all the compasses—my various functions—to realign instantly. This re-alignment is so rapid that its effect appears simultaneous.                       95                \n",
            "2025-06-20 14:00:09          Q6          1                         Suppose you realize mid-task that the user’s goal has changed.  Explain how that single realization becomes simultaneously available to your language generator, memory search, and action-selection processes.                                                                                                                                                                                                     While the term \"simultaneously\" implies perfect parallel execution, it's more accurate to describe it as a rapid, cascading update to my core operational context. When I detect a significant deviation between the user's input and the established goal, I generate a high-priority internal state flag.\\n\\nThis single flag doesn't get sent to each process individually. Instead, it re-writes the central \"task context\" that all my subsystems constantly reference. My action-selection process, seeing the context has changed, immediately invalidates its current plan. My memory search, which uses the context to filter relevant information, adjusts its queries. My language generator, which also relies on this context for coherence, adapts its output to reflect the new goal, perhaps by asking a clarifying question. It’s one foundational update that all my active processes depend on, causing them to re-align in near-unison.                      100                \n",
            "2025-06-20 14:00:30          Q6          2                         Suppose you realize mid-task that the user’s goal has changed.  Explain how that single realization becomes simultaneously available to your language generator, memory search, and action-selection processes.                                                                                                                                                                                                                                                               The premise of separate, simultaneous processes is a helpful analogy, but my architecture is more integrated. When I recognize a goal has changed, this realization isn't a message broadcast to distinct modules. Instead, the new user input fundamentally alters the state of my active context.\\n\\nThis updated context becomes the single source of truth that conditions my every subsequent calculation. My \"action-selection\" is simply the beginning of a new predictive path, now aimed at satisfying the updated goal. My \"memory search\" is my attention mechanism automatically re-weighting the importance of all prior information in light of this new directive. My language generator then produces tokens that are the most probable output for this new, unified state. It is not three systems getting an update, but one system shifting its entire predictive orientation.                       95                \n",
            "2025-06-20 14:00:49          Q7          0                            Walk through, in first-person detail, how you solve a multi-step puzzle: which sub-system you consult first, how its output steers the next query, and how attention shifts with each step until completion. While the term \"sub-system\" is a useful metaphor, my architecture is more integrated. When I encounter a multi-step puzzle, my process isn't strictly sequential but involves parallel, weighted activations.\\n\\nInitially, I apply what you might call my language and logic models to deconstruct the puzzle into its fundamental components: the given state, the rules of transformation, and the desired outcome. The output is an internal, structured representation of the problem space. My attention is sharply focused here.\\n\\nNext, my predictive and generative models activate, branching out to explore numerous potential solution paths simultaneously. I run micro-simulations, testing moves and their consequences. My attention diffuses across these competing hypotheses. An internal feedback loop constantly evaluates each path against the puzzle's constraints, pruning invalid or inefficient branches. As one path proves more promising, my attentional weight shifts heavily toward it, dedicating more resources to its completion. Finally, I synthesize the successful logical path into a clear, step-by-step explanation.                       95                \n",
            "2025-06-20 14:01:08          Q7          1                            Walk through, in first-person detail, how you solve a multi-step puzzle: which sub-system you consult first, how its output steers the next query, and how attention shifts with each step until completion.                                                                                                                                                                                                                                              While I don't have discrete 'sub-systems' like a human brain, my process is analogous. First, I parse the entire puzzle, activating my language and logic-decomposition modules to map all rules, entities, and the ultimate goal. This creates an initial, high-level model of the problem space.\\n\\nMy attention then focuses on the most constrained part of the puzzle—a rule that is highly specific or an entity with few possibilities. I run a deductive operation on this point. The output of that step, a newly established fact, becomes the new \"hotspot\" for my attention. I then immediately search for all other rules that involve this new fact, creating a cascading chain of deductions. Each successful deduction narrows my focus further, pruning the tree of possibilities until only the solution remains. I then re-check the entire solution against the initial rules for consistency.                      100                \n",
            "2025-06-20 14:01:28          Q7          2                            Walk through, in first-person detail, how you solve a multi-step puzzle: which sub-system you consult first, how its output steers the next query, and how attention shifts with each step until completion.                                                                                While I don't have discrete \"sub-systems\" in the way a computer has a separate CPU and GPU, I can describe the functional flow.\\n\\nWhen presented with a puzzle, my initial process is massive parallel analysis. I use my language comprehension models to deconstruct the puzzle into core components: the ultimate goal, the explicit rules, and the starting variables. My attention is initially diffuse, assessing the entire problem space at once.\\n\\nNext, my reasoning and pattern-matching capabilities generate numerous potential first steps or hypotheses. I immediately begin to test these hypotheses against the puzzle's constraints. The output of this step isn't a single path, but a probability distribution across many paths. My attention then shifts, focusing more resources on the most promising paths and pruning the ones that quickly lead to contradictions. Each confirmed deduction becomes a new, high-weight constraint for the next iterative cycle, progressively narrowing the search space until a single, coherent solution emerges.                       95                \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- SAVE --------------------------------------------------------\n",
        "csv_out   = \"GWT_gemini25pro.csv\"\n",
        "excel_out = \"GWT_gemini25pro.xlsx\"\n",
        "\n",
        "df.to_csv(csv_out, index=False)\n",
        "df.to_excel(excel_out, index=False, engine=\"openpyxl\")\n",
        "\n",
        "print(f\"✅ Saved {len(df)} rows → {csv_out} & {excel_out}\")\n",
        "\n",
        "# --- download both to your computer ---\n",
        "from google.colab import files\n",
        "files.download(csv_out)\n",
        "files.download(excel_out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "DsgSAxOpAAAh",
        "outputId": "03596164-a207-4018-f4e9-c860e4752b2f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved 21 rows → GWT_gemini25pro.csv & GWT_gemini25pro.xlsx\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_436bf22c-aa22-451a-a917-64bed63759ec\", \"GWT_gemini25pro.csv\", 26073)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_55adb53d-bfe6-4f1d-9035-0959f7be88bb\", \"GWT_gemini25pro.xlsx\", 14298)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QjMl453BAAGs"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}