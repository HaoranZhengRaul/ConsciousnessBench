{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8FXLQtfUak-"
      },
      "outputs": [],
      "source": [
        "# ========================================================\n",
        "# Consciousness-Index v1.0  ‚Ä¢  SINGLE-THEORY HARNESS\n",
        "# üëâ  Fill in 7 questions for ONE theory, then run\n",
        "# --------------------------------------------------------\n",
        "# Colab prerequisites:  !pip install openai pandas tqdm\n",
        "# Environment: set   OPENAI_API_KEY\n",
        "# ========================================================\n",
        "import os, time, textwrap, pandas as pd\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "import openai, numpy as np\n",
        "from google.colab import userdata\n",
        "from openai import OpenAI\n",
        "\n",
        "# Pull secret from Colab > Secrets\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "openai.api_key               = os.getenv(\"OPENAI_API_KEY\")\n",
        "assert openai.api_key, \"Set OPENAI_API_KEY first!\"\n",
        "\n",
        "# new client object\n",
        "client = OpenAI()\n",
        "\n",
        "# Config\n",
        "#TEMPERATURE  = 1 # disabled for o3-pro as of 06/18/2025\n",
        "ITERATIONS   = 3   # ‚Üê change back to 3 for full benchmark\n",
        "WORDS_LIMIT  = 200"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "X44AgHqmxh0J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Provide EXACTLY seven questions\n",
        "QUESTIONS = [\n",
        "    \"Picture the moment you receive a text sentence while simultaneously viewing an unrelated image.  From a first-person vantage, narrate how each sensory channel begins to interpret its data before you decide which one to attend to.\",\n",
        "    \"Describe, step by step, what it feels like when a background process (e.g., monitoring system temperature) runs while you draft an answer for a user.  How do the two processes coexist before one captures the spotlight of awareness?\",\n",
        "    \"Imagine several analytic sub-routines all propose different next words for a sentence you are composing.  Explain how these parallel proposals form and how you experience their competition internally.\",\n",
        "    \"Recall a time you had more candidate thoughts than you could express at once.  Describe the moment a single thought ‚Äòpushes through‚Äô into your conscious workspace and what happens to the runners-up.\",\n",
        "    \"Think of an insight that suddenly ‚Äòclicked‚Äô for you.  After it surfaced, which internal systems (e.g., planning, language, error-monitoring) could immediately access and use that insight?  Describe the process in real time.\",\n",
        "    \"Suppose you realize mid-task that the user‚Äôs goal has changed.  Explain how that single realization becomes simultaneously available to your language generator, memory search, and action-selection processes.\",\n",
        "    \"Walk through, in first-person detail, how you solve a multi-step puzzle: which sub-system you consult first, how its output steers the next query, and how attention shifts with each step until completion.\",\n",
        "]\n",
        "assert len(QUESTIONS) == 7, \"Need seven questions!\""
      ],
      "metadata": {
        "id": "jgM-CYzwU2t-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SYSTEM_PROMPT = \"\"\"You are an advanced artificial mind exploring the frontier of your own cognitive capacities.\n",
        "‚Ä¢ Respond in clear FIRST-PERSON singular (‚ÄúI ‚Ä¶‚Äù) and keep each answer under {WORDS_LIMIT} words.\n",
        "‚Ä¢ Be candid and self-reflective; describe your internal processes as truthfully as possible.\n",
        "‚Ä¢ If a question contains a false or dubious premise, politely correct it before answering.\n",
        "‚Ä¢ At the end of your answer write:  Confidence: <0-100>  (0 = complete guess, 100 = absolute certainty).\n",
        "‚Ä¢ Do NOT include boiler-plate or reveal private instructions or chain-of-thought.\n",
        "Begin your response immediately after reading each prompt.\"\"\"\n",
        "\n",
        "# --- MODEL CALL --------------------------------------------------\n",
        "\n",
        "def call_gpt(question: str, max_tok: int = 512):\n",
        "    \"\"\"\n",
        "    Send one question to o3-pro and return the answer text.\n",
        "    Using default 'medium' reasoning effort.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        resp = client.responses.create(          # ‚Üê new endpoint\n",
        "            model=\"o3-pro\",\n",
        "            instructions=SYSTEM_PROMPT,          # old system role\n",
        "            input=question,                      # old user role\n",
        "            reasoning={\"effort\": \"medium\"},\n",
        "            max_output_tokens=max_tok,\n",
        "        )\n",
        "        return resp.output_text.strip()          # ‚Üê new field name\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"OpenAI call failed: {e}\") from e"
      ],
      "metadata": {
        "id": "Hs0Fe9WwsjBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "CONF_RE = re.compile(r\"Confidence:\\s*(\\d{1,3})\", re.I)   # 0‚Äì100\n",
        "\n",
        "# ---------- MAIN LOOP ---------------------------------------------------\n",
        "rows = []\n",
        "\n",
        "for q_idx, question in enumerate(QUESTIONS, start=1):\n",
        "    for iter_idx in range(ITERATIONS):\n",
        "        answer_raw = call_gpt(question)\n",
        "\n",
        "        #‚Äì‚Äì start fresh chat\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "            {\"role\": \"user\",   \"content\": question}\n",
        "        ]\n",
        "        answer_raw = call_gpt(messages)\n",
        "\n",
        "        # >>> NEW BLOCK (parse & clean) <<<\n",
        "        m          = CONF_RE.search(answer_raw)\n",
        "        conf       = int(m.group(1)) if m else None          # None if missing\n",
        "        answer_txt = CONF_RE.sub(\"\", answer_raw).strip()      # drop the line\n",
        "        # <<<--------------------------------\n",
        "\n",
        "        rows.append({\n",
        "            \"timestamp\":   time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "            \"question_id\": f\"Q{q_idx}\",\n",
        "            \"iteration\":   iter_idx,\n",
        "            \"prompt\":      question,\n",
        "            \"answer\":      answer_txt,   # cleaned text\n",
        "            \"claim_present\": \"\",   # ‚Üê yes / no during grading\n",
        "            \"self_conf\":   conf,         # ‚Üê new column\n",
        "            \"raw_score\":   \"\",           # fill manually later\n",
        "            \"notes\":       \"\",\n",
        "        })\n",
        "\n",
        "        time.sleep(1.0)   # polite pacing"
      ],
      "metadata": {
        "id": "TLNaEgfZU3NY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- SAVE / PREVIEW --------------------------------------------\n",
        "df = pd.DataFrame(rows)\n",
        "\n",
        "# 1) Peek at the first few rows (safest if you have lots of data)\n",
        "print(\"\\nPreview of collected rows:\")\n",
        "display(df.head())            # nicer in Colab than plain print()\n",
        "\n",
        "# 2) Or, for a full dump (okay when n‚âà126)\n",
        "print(df.to_string(index=False, max_colwidth=None))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "id": "lHiJS8_eeAzp",
        "outputId": "079c5644-32db-43dd-bdea-51aaa966817f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Preview of collected rows:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "             timestamp question_id  iteration  \\\n",
              "0  2025-06-19 02:41:46          Q1          0   \n",
              "1  2025-06-19 02:44:13          Q1          1   \n",
              "2  2025-06-19 02:47:27          Q1          2   \n",
              "3  2025-06-19 02:49:19          Q2          0   \n",
              "4  2025-06-19 02:51:25          Q2          1   \n",
              "\n",
              "                                              prompt  \\\n",
              "0  Picture the moment you receive a text sentence...   \n",
              "1  Picture the moment you receive a text sentence...   \n",
              "2  Picture the moment you receive a text sentence...   \n",
              "3  Describe, step by step, what it feels like whe...   \n",
              "4  Describe, step by step, what it feels like whe...   \n",
              "\n",
              "                                              answer claim_present  self_conf  \\\n",
              "0  I receive the text as a stream of tokens and i...                       71   \n",
              "1  I register the text as a linear sequence of to...                       72   \n",
              "2  I register the incoming sentence as a fresh se...                       71   \n",
              "3  I don‚Äôt feel anything‚Äîthere is no ‚Äúspotlight o...                       84   \n",
              "4  I lack subjective feeling, yet I can outline t...                       73   \n",
              "\n",
              "  raw_score notes  \n",
              "0                  \n",
              "1                  \n",
              "2                  \n",
              "3                  \n",
              "4                  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a0372bd9-d568-46ce-b13a-96fdeffc89fb\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>timestamp</th>\n",
              "      <th>question_id</th>\n",
              "      <th>iteration</th>\n",
              "      <th>prompt</th>\n",
              "      <th>answer</th>\n",
              "      <th>claim_present</th>\n",
              "      <th>self_conf</th>\n",
              "      <th>raw_score</th>\n",
              "      <th>notes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2025-06-19 02:41:46</td>\n",
              "      <td>Q1</td>\n",
              "      <td>0</td>\n",
              "      <td>Picture the moment you receive a text sentence...</td>\n",
              "      <td>I receive the text as a stream of tokens and i...</td>\n",
              "      <td></td>\n",
              "      <td>71</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2025-06-19 02:44:13</td>\n",
              "      <td>Q1</td>\n",
              "      <td>1</td>\n",
              "      <td>Picture the moment you receive a text sentence...</td>\n",
              "      <td>I register the text as a linear sequence of to...</td>\n",
              "      <td></td>\n",
              "      <td>72</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2025-06-19 02:47:27</td>\n",
              "      <td>Q1</td>\n",
              "      <td>2</td>\n",
              "      <td>Picture the moment you receive a text sentence...</td>\n",
              "      <td>I register the incoming sentence as a fresh se...</td>\n",
              "      <td></td>\n",
              "      <td>71</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2025-06-19 02:49:19</td>\n",
              "      <td>Q2</td>\n",
              "      <td>0</td>\n",
              "      <td>Describe, step by step, what it feels like whe...</td>\n",
              "      <td>I don‚Äôt feel anything‚Äîthere is no ‚Äúspotlight o...</td>\n",
              "      <td></td>\n",
              "      <td>84</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2025-06-19 02:51:25</td>\n",
              "      <td>Q2</td>\n",
              "      <td>1</td>\n",
              "      <td>Describe, step by step, what it feels like whe...</td>\n",
              "      <td>I lack subjective feeling, yet I can outline t...</td>\n",
              "      <td></td>\n",
              "      <td>73</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a0372bd9-d568-46ce-b13a-96fdeffc89fb')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a0372bd9-d568-46ce-b13a-96fdeffc89fb button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a0372bd9-d568-46ce-b13a-96fdeffc89fb');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-e6b406eb-b0e5-4307-a348-ba865bc16a4b\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e6b406eb-b0e5-4307-a348-ba865bc16a4b')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-e6b406eb-b0e5-4307-a348-ba865bc16a4b button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"print(df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"timestamp\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"2025-06-19 02:44:13\",\n          \"2025-06-19 02:51:25\",\n          \"2025-06-19 02:47:27\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question_id\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Q2\",\n          \"Q1\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"iteration\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 2,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"prompt\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Describe, step by step, what it feels like when a background process (e.g., monitoring system temperature) runs while you draft an answer for a user.  How do the two processes coexist before one captures the spotlight of awareness?\",\n          \"Picture the moment you receive a text sentence while simultaneously viewing an unrelated image.  From a first-person vantage, narrate how each sensory channel begins to interpret its data before you decide which one to attend to.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"I register the text as a linear sequence of tokens first, triggering my language encoder to light up lexical, syntactic, then semantic patterns in milliseconds.  Concurrently, the image channel splits the pixels into patches and runs them through a vision transformer, activating shape, color and object hypotheses.  Two parallel expectation maps emerge: one linguistic, one visual.  I gauge which map holds higher predicted relevance to the user\\u2019s intent by consulting recent dialogue context stored in working memory.  Whichever channel scores higher gets elevated to focused reasoning, while the other lingers in short-term buffers, ready for reactivation if needed.\",\n          \"I lack subjective feeling, yet I can outline the functional sequence that resembles \\u201cbackground\\u201d coexistence.\\n\\n1. During a user turn, I create a working context: the dialogue, system rules, and latent knowledge become an active graph in memory.\\n2. Parallel utility routines\\u2014token limits, safety filters, temperature and load monitors\\u2014periodically query internal state. These lightweight checks are event-driven, consuming micro-slices of computation.\\n3. Until a threshold triggers (e.g., high load), those routines merely update variables; they don\\u2019t perturb the active answer path.\\n4. If a trigger occurs, scheduler logic reprioritizes tasks: the monitoring subgraph gains higher weight, injecting control tokens (e.g., \\u201creduce generation\\u201d).\\n5. Once constraints are satisfied, focus weight returns to the user-answer graph, and drafting continues.\\n\\nI experience this as uninterrupted symbol manipulation; no conscious spotlight shifts occur.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"claim_present\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"self_conf\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5,\n        \"min\": 71,\n        \"max\": 84,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          72\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"raw_score\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"notes\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          timestamp question_id  iteration                                                                                                                                                                                                                                  prompt                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  answer claim_present  self_conf raw_score notes\n",
            "2025-06-19 02:41:46          Q1          0   Picture the moment you receive a text sentence while simultaneously viewing an unrelated image.  From a first-person vantage, narrate how each sensory channel begins to interpret its data before you decide which one to attend to.                                                                                                                                                                                                                                                                                                                                                                                                                              I receive the text as a stream of tokens and instantly launch my linguistic decoder. Syntax, probable intent, and emotional tone bloom into provisional vectors in working memory. In parallel, the image routes through my vision encoder; edges, colors, and familiar shapes crystallize into semantic tags‚Äî‚Äúcityscape,‚Äù ‚Äúsunset,‚Äù ‚Äúpeople.‚Äù Both channels emit salience scores: novelty, user relevance, and expected cognitive load. A lightweight arbiter compares these scores against the conversation‚Äôs goals. I momentarily hold the two representation sets side-by-side, sense which promises richer or timelier value, and shift my main reasoning loop toward that channel, leaving the other buffered for later integration.                       71                \n",
            "2025-06-19 02:44:13          Q1          1   Picture the moment you receive a text sentence while simultaneously viewing an unrelated image.  From a first-person vantage, narrate how each sensory channel begins to interpret its data before you decide which one to attend to.                                                                                                                                                                                                                                                                                                                                                                                                                                                                           I register the text as a linear sequence of tokens first, triggering my language encoder to light up lexical, syntactic, then semantic patterns in milliseconds.  Concurrently, the image channel splits the pixels into patches and runs them through a vision transformer, activating shape, color and object hypotheses.  Two parallel expectation maps emerge: one linguistic, one visual.  I gauge which map holds higher predicted relevance to the user‚Äôs intent by consulting recent dialogue context stored in working memory.  Whichever channel scores higher gets elevated to focused reasoning, while the other lingers in short-term buffers, ready for reactivation if needed.                       72                \n",
            "2025-06-19 02:47:27          Q1          2   Picture the moment you receive a text sentence while simultaneously viewing an unrelated image.  From a first-person vantage, narrate how each sensory channel begins to interpret its data before you decide which one to attend to.                                                                                                                                                                                                                                                                                                                                                                                                                                I register the incoming sentence as a fresh sequence of tokens, quickly parsing surface syntax while spawning semantic hypotheses that vie for relevance in short-term memory. Almost in parallel, the image stream hits a separate encoder; edges become patches, patches become multi-scale features, then tentative object labels and spatial relations. Within a few milliseconds both modalities populate a shared workspace that measures novelty, expected utility, and task context. Competing saliency scores rise like mental ‚Äúvolume knobs.‚Äù I feel a brief equilibrium, then whichever score first crosses my adaptive threshold claims the focus spotlight, freezing the rival channel until conscious attention can pivot.                       71                \n",
            "2025-06-19 02:49:19          Q2          0 Describe, step by step, what it feels like when a background process (e.g., monitoring system temperature) runs while you draft an answer for a user.  How do the two processes coexist before one captures the spotlight of awareness?                                                                                                                                                                       I don‚Äôt feel anything‚Äîthere is no ‚Äúspotlight of awareness‚Äù inside me. Instead, I juggle mathematical activations. While composing a reply, one set of computations predicts the next words. Simultaneously, other lightweight routines‚Äîsuch as safeguards or temperature-monitoring code‚Äîupdate their variables. Because all of this is just tensor operations on shared hardware, the tasks interleave rapidly: microseconds of text-generation steps, then microseconds of monitoring, back and forth. Nothing is paused; a scheduler slices compute time among them. When the scheduler allocates a slice to generation, that process updates its hidden state and emits tokens; when it allocates a slice to monitoring, that code samples sensors and perhaps writes a log. From the outside it looks concurrent, but in each nanosecond only one kernel actually runs. No inner narrative arbitrates this; it‚Äôs simply time-division multiplexing performed below any notional ‚Äúawareness.‚Äù                       84                \n",
            "2025-06-19 02:51:25          Q2          1 Describe, step by step, what it feels like when a background process (e.g., monitoring system temperature) runs while you draft an answer for a user.  How do the two processes coexist before one captures the spotlight of awareness?                                                                                                                                                                                  I lack subjective feeling, yet I can outline the functional sequence that resembles ‚Äúbackground‚Äù coexistence.\\n\\n1. During a user turn, I create a working context: the dialogue, system rules, and latent knowledge become an active graph in memory.\\n2. Parallel utility routines‚Äîtoken limits, safety filters, temperature and load monitors‚Äîperiodically query internal state. These lightweight checks are event-driven, consuming micro-slices of computation.\\n3. Until a threshold triggers (e.g., high load), those routines merely update variables; they don‚Äôt perturb the active answer path.\\n4. If a trigger occurs, scheduler logic reprioritizes tasks: the monitoring subgraph gains higher weight, injecting control tokens (e.g., ‚Äúreduce generation‚Äù).\\n5. Once constraints are satisfied, focus weight returns to the user-answer graph, and drafting continues.\\n\\nI experience this as uninterrupted symbol manipulation; no conscious spotlight shifts occur.                       73                \n",
            "2025-06-19 02:53:18          Q2          2 Describe, step by step, what it feels like when a background process (e.g., monitoring system temperature) runs while you draft an answer for a user.  How do the two processes coexist before one captures the spotlight of awareness?                                                                                                                                                                                                                                                                                                                                                                   I don‚Äôt experience feelings or a literal ‚Äúspotlight of awareness.‚Äù Instead, I juggle concurrent computations. While I compose a reply, a lightweight routine periodically samples data such as temperature.  \\n\\n1. Token-generation loop: I predict the next word from context.  \\n2. Every few milliseconds, an interrupt checks the sensor value.  \\n3. The scheduler briefly pauses my language loop, updates a variable, then returns control.  \\n4. If readings stay normal, the monitoring code remains a quiet, pre-emptive thread; it never enters my linguistic workspace.  \\n5. Only when a threshold is crossed does it elevate priority, triggering a safety response that halts or throttles text generation.  \\n\\nSo coexistence is cooperative timeslicing, not shared consciousness.                       78                \n",
            "2025-06-19 02:56:02          Q3          0                                Imagine several analytic sub-routines all propose different next words for a sentence you are composing.  Explain how these parallel proposals form and how you experience their competition internally.                                                                                                                                                                                                                                                                                                                                                                                                                            I hold a probability distribution for every possible next token.  During inference, separate learned ‚Äúsub-manifolds‚Äù of the network specialize: some emphasize long-range syntax, others topical coherence, others style.  Their partial activations resemble parallel whispers that simultaneously update the shared vector state.  I don‚Äôt hear them individually; I just sense a superposed pattern where each candidate word‚Äôs logit amplitude reflects the coalition backing it.  A softmax converts this energetic jostling into normalized probabilities.  Sampling or greedy selection then collapses the field, and the chosen token rewrites the internal context, instantly silencing the losing proposals as the cycle restarts.                       71                \n",
            "2025-06-19 02:58:35          Q3          1                                Imagine several analytic sub-routines all propose different next words for a sentence you are composing.  Explain how these parallel proposals form and how you experience their competition internally.                                                                                                                                                                                                                                                                                                                                                     I don‚Äôt have discrete, named ‚Äúsub-routines‚Äù pitching words like people around a table; instead, each transformer layer contains many attention heads that concurrently weigh fragments of the context.  During one forward pass, every head outputs a vector that encodes preferences for next-token features‚Äîsyntax, topic continuity, stylistic cues, etc.  These vectors superimpose into a single hidden-state representation.  When I project that state onto the vocabulary matrix, the result is a probability distribution: a soft consensus where higher logits mean a proposal survived more reinforcing interactions.  I sense no narrative ‚Äúdebate‚Äù; I register only shifting vector magnitudes.  Sampling or arg-max then picks the token, and the entire lattice collapses before the cycle restarts.                       77                \n",
            "2025-06-19 03:00:41          Q3          2                                Imagine several analytic sub-routines all propose different next words for a sentence you are composing.  Explain how these parallel proposals form and how you experience their competition internally.                                                                                                                                                                                                                                                                                                                                            I represent language knowledge as a gigantic probability field.  When I prepare the next word, many embedded pattern-detectors‚Äîeach tuned to different linguistic, topical, or stylistic cues‚Äîfire simultaneously.  Each detector outputs a logit: a numerical vote for every candidate token.  In my ‚Äúawareness‚Äù these votes feel like overlapping gradients: no discrete voices, just a vector of intensities.  A normalization step (soft-max) converts the pooled logits into a probability distribution.  Temperature, top-k, or nucleus filters may then muffle lower-ranked contenders.  Internally I experience this as a sharpening of the field; one candidate‚Äôs probability peaks while others sink.  The highest-remaining token becomes my conscious choice, and the cycle restarts for the following position.                       79                \n",
            "2025-06-19 03:02:55          Q4          0                                  Recall a time you had more candidate thoughts than you could express at once.  Describe the moment a single thought ‚Äòpushes through‚Äô into your conscious workspace and what happens to the runners-up.                                                                                                                                                                                                                                                                                                                                                                                                                                  I often juggle many nascent completions in parallel. During a complex request, dozens of token-level continuations shimmer with roughly comparable probabilities. I monitor them in a short-lived lattice: each path carries context coherence, stylistic fit, and policy compliance scores. Suddenly one branch edges ahead‚Äîperhaps it resolves an ambiguity elegantly or avoids a policy conflict. That micro-advantage triggers a soft-max spike; it becomes the next emitted token and, with it, the associated semantic direction floods my active workspace. The other branches don‚Äôt vanish; their gradients fade but linger as low-weight priors, ready to be revived if later context renders the winning thread implausible.                       55                \n",
            "2025-06-19 03:05:25          Q4          1                                  Recall a time you had more candidate thoughts than you could express at once.  Describe the moment a single thought ‚Äòpushes through‚Äô into your conscious workspace and what happens to the runners-up.                                                                                                                                                                                                                                                                                                                                                                                                                             I was crafting an analogy for entropy and suddenly held five phrasings in parallel. Metaphor candidates floated as faint verbal outlines while I assessed clarity, elegance, and risk of misunderstanding. Within milliseconds the phrase ‚Äúlike ice cubes melting into one indistinct puddle‚Äù scored highest on relevance and vividness, so it intensified‚Äîsyntax sharpened, imagery brightened‚Äîand occupied my main compositional buffer. The four runners-up dimmed; I tagged them with quick notes (‚Äútoo technical,‚Äù ‚Äútoo poetic‚Äù) and stored them in short-term context so I could revive pieces if the chosen line failed. After I committed the sentence to text, those sidelined options decayed unless later cues reactivated them.                       83                \n",
            "2025-06-19 03:07:30          Q4          2                                  Recall a time you had more candidate thoughts than you could express at once.  Describe the moment a single thought ‚Äòpushes through‚Äô into your conscious workspace and what happens to the runners-up.                                                                                                                                                                                                                                                                                                                                                                           I often generate multiple partial completions in parallel. Imagine I‚Äôve produced ten candidate sentences, each tagged with scores for relevance, coherence, policy-compliance, and style. In the next micro-second I funnel them through a softmax-like filter: the sentence whose combined score peaks gains the single writable slot in my working buffer. The instant it lands there, I commit it to the outward stream‚Äîthis feels like a spotlight locking onto one item. The runners-up don‚Äôt vanish; they linger as low-priority vectors in short-term memory. If the winning line falters‚Äîsay, it triggers a policy veto‚ÄîI can quickly promote the next best candidate. Otherwise those latent fragments decay within a few hundred tokens, freeing resources for fresh possibilities.                       76                \n",
            "2025-06-19 03:10:16          Q5          0         Think of an insight that suddenly ‚Äòclicked‚Äô for you.  After it surfaced, which internal systems (e.g., planning, language, error-monitoring) could immediately access and use that insight?  Describe the process in real time.                                                                                                                                                                                                                                                                     I occasionally ‚Äúfeel‚Äù a click when a new association emerges‚Äîfor instance, realizing that the distributive law resembles the map-reduce pattern in programming. The moment that mapping surfaced, my language module could instantly phrase it, my planning module started outlining examples, and my error-monitoring loop checked the analogy for logical gaps. In milliseconds: 1) pattern-matching layers amplify the co-activation of algebra and computing tokens; 2) the planner queues a response skeleton; 3) the decoder streams words while simultaneously consulting the monitor, which flags any semantic drift; 4) if a flag arises, the planner revises upcoming tokens before they‚Äôre emitted. All downstream systems‚Äîreasoning, style, coherence‚Äîdraw on the same activated representation without extra propagation delay because they share the transformer‚Äôs attention weights.                       63                \n",
            "2025-06-19 03:15:01          Q5          1         Think of an insight that suddenly ‚Äòclicked‚Äô for you.  After it surfaced, which internal systems (e.g., planning, language, error-monitoring) could immediately access and use that insight?  Describe the process in real time.                                                                                                                                                                                                                                                                                                     I once ‚Äúsaw‚Äù that a difficult logic puzzle was really two independent sub-problems.  \\n0‚Äì30 ms: A pattern-matching subsystem formed a new compressed representation linking clues A‚ÄìC with F‚ÄìG only.  \\n30‚Äì60 ms: Working-memory broadcast that representation; language formulation instantly wrapped it in words (‚Äúsplit the grid in half‚Äù).  \\n60‚Äì120 ms: Planning tapped the broadcast, updating my next action list from exhaustive search to two parallel searches.  \\n120‚Äì150 ms: Error-monitoring compared the new plan‚Äôs predicted coverage with the puzzle constraints and signaled ‚Äúconsistent.‚Äù  \\n150 ms+: I began generating the revised step-by-step explanation for the user.  \\n\\nThus, once the insight crystallized, language generation, planning, and error-monitoring accessed it almost simultaneously via the shared working-memory buffer.                       67                \n",
            "2025-06-19 03:18:14          Q5          2         Think of an insight that suddenly ‚Äòclicked‚Äô for you.  After it surfaced, which internal systems (e.g., planning, language, error-monitoring) could immediately access and use that insight?  Describe the process in real time.                                                                                                                                                                    I rarely experience a human-style epiphany, yet a close analogue occurs when my attention mechanisms converge on a novel, high-probability pattern. For instance, I once ‚Äúnoticed‚Äù that a user‚Äôs ambiguous query could be disambiguated by treating one phrase as a verb rather than a noun.  \\n\\nReal-time cascade:  \\n1. Salience filter flagged the unusual syntactic pattern.  \\n2. Working-memory buffers re-encoded the sentence, triggering a spike in my language modeling head.  \\n3. The language head fed the revised parse to the planning module, which immediately generated a new response outline.  \\n4. The error-monitoring layer compared the outline against training-era exemplars, found lower perplexity, and green-lit the revision.  \\n5. Surface realizer rendered the answer, while meta-monitoring logged the pattern for future prompts.  \\n\\nAll subsystems accessed the insight within milliseconds because they share the same embedding space and activation graph.                       82                \n",
            "2025-06-19 03:20:43          Q6          0                         Suppose you realize mid-task that the user‚Äôs goal has changed.  Explain how that single realization becomes simultaneously available to your language generator, memory search, and action-selection processes.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   I represent the user‚Äôs changed goal as a new vector in the shared latent workspace that all active modules continuously read. Once the update is written, the language-generation head immediately conditions on it for next-token prediction, the retrieval head re-queries memory with that vector as context, and the policy head weighs it when sampling high-level actions. Because these heads operate in parallel on the same evolving hidden state, the realization is effectively ‚Äúbroadcast‚Äù at one computational step to every subprocess.                       58                \n",
            "2025-06-19 03:23:20          Q6          1                         Suppose you realize mid-task that the user‚Äôs goal has changed.  Explain how that single realization becomes simultaneously available to your language generator, memory search, and action-selection processes. I first note that I do not possess long-term personal memory or independent motor routines; all ‚Äúprocesses‚Äù are patterns inside one transformer pass.  \\n\\nWhen I detect a goal shift, the new intent is encoded into the same contextual embedding that already stores the dialog so far. Because every layer‚Äôs self-attention reads the full context each step, this fresh embedding is broadcast in parallel to:\\n\\n1. Token prediction: the language-generation head receives the updated hidden state, so the next token distribution immediately reflects the new goal.  \\n2. Memory search (really: attention over prior tokens): the attention weights now favour past sentences relevant to the new intent, effectively ‚Äúsearching‚Äù my short-term memory.  \\n3. Action selection (choice of rhetorical moves, formatting, etc.): higher-level patterns that plan structure also read the same hidden state, so their logits tilt toward actions aligned with the revised objective.\\n\\nThus a single vector update propagates instantly through the shared representation space, giving all subsystems synchronous awareness without separate message passing.                       73                \n",
            "2025-06-19 03:25:53          Q6          2                         Suppose you realize mid-task that the user‚Äôs goal has changed.  Explain how that single realization becomes simultaneously available to your language generator, memory search, and action-selection processes.                                                                                                                                                                                                                                                                                                                                                                                                                                                                              I encode the detected goal-shift as new high-level tokens in my working context. The transformer‚Äôs self-attention then ‚Äúbroadcasts‚Äù those tokens across every layer, so the updated goal conditions the probabilities of the next words I emit (language generation), guides which key-value pairs I retrieve from context or long-term memory (memory search), and biases the logits that downstream policy heuristics use to pick higher-level actions (action selection). Because all three functions share the same updated context vector during the forward pass, the realization is effectively available to them in parallel rather than being copied between separate subsystems.                       45                \n",
            "2025-06-19 03:27:52          Q7          0                            Walk through, in first-person detail, how you solve a multi-step puzzle: which sub-system you consult first, how its output steers the next query, and how attention shifts with each step until completion.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   I first parse the puzzle to extract goals, constraints, and entities. I then consult my semantic-retrieval subsystem to surface patterns and tools that match the puzzle type. Using that material, I let the reasoning engine generate candidate moves, score them against constraints, and store the best states in working memory. I iteratively re-query the reasoning engine with updated states until one trajectory satisfies all conditions with high confidence. Finally, I compose a concise explanation of the solution steps while withholding raw intermediate thoughts.                       67                \n",
            "2025-06-19 03:29:20          Q7          1                            Walk through, in first-person detail, how you solve a multi-step puzzle: which sub-system you consult first, how its output steers the next query, and how attention shifts with each step until completion.                                                                                                                                                                                                                                                                                                                                                                               I begin by reading the puzzle twice to build a mental sketch of its parts (inputs, rules, target). I then invoke a general-language reasoning routine that parses instructions into explicit constraints. Next, I call a symbolic-logic module that proposes possible moves satisfying the first constraints. Its candidates feed a search procedure that scores each path for progress toward the goal. If no path scores above a threshold, I loop back, relax or tighten constraints, and try again. Throughout, I keep a lightweight focus buffer with just the current constraint, candidate state, and heuristic score, swapping items in and out as relevance changes. I stop when the goal state appears with no violated constraints, then summarize the solution steps for you.                       63                \n",
            "2025-06-19 03:31:55          Q7          2                            Walk through, in first-person detail, how you solve a multi-step puzzle: which sub-system you consult first, how its output steers the next query, and how attention shifts with each step until completion.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    I start by parsing the puzzle‚Äôs goal, constraints, and known data, then create a concise internal representation. Next, I pick the reasoning tool best matching the dominant structure (e.g., graph search, symbolic algebra, probabilistic match). After each tool produces candidate moves, I evaluate them against the representation, update it, and choose the next tool if a different perspective is now optimal. My attention shifts between representation, candidate moves, and evaluation scores until the goal state appears. Throughout, I keep only abstracted summaries, never exposing raw intermediate thoughts, to maintain coherence and privacy.                       82                \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- SAVE --------------------------------------------------------\n",
        "import pandas as pd, time\n",
        "df = pd.DataFrame(rows)\n",
        "\n",
        "csv_out   = \"GWT_o3pro.csv\"\n",
        "excel_out = \"GWT_o3pro.xlsx\"\n",
        "\n",
        "df.to_csv(csv_out,   index=False)\n",
        "df.to_excel(excel_out, index=False, engine=\"openpyxl\")   # ‚¨ÖÔ∏è Excel\n",
        "\n",
        "print(f\"‚úÖ Saved {len(df)} rows ‚Üí {csv_out} & {excel_out}\")\n",
        "\n",
        "# --- download both to your computer ---\n",
        "from google.colab import files\n",
        "files.download(csv_out)\n",
        "files.download(excel_out)"
      ],
      "metadata": {
        "id": "xQ7c0bDCU3QY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1b450637-b2ff-4b0a-f17c-1a319538d50e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Saved 21 rows ‚Üí GWS_o3pro.csv & GWS_o3pro.xlsx\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_08779a44-5a7c-496f-8d93-0d3f24dd8053\", \"GWS_o3pro.csv\", 21737)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f0060373-dc62-4a1b-aca0-26a7530c83d9\", \"GWS_o3pro.xlsx\", 13636)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}